{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\user\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\user\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\user\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\user\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\user\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\user\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\user\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\user\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\user\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\user\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\user\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\user\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# LAB1-1\n",
    "# LAB1-1\n",
    "# LAB1-1\n",
    "# LAB1-1\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import pickle\n",
    "import h5py\n",
    "import glob\n",
    "import time\n",
    "from random import shuffle\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten,Dropout,Softmax\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D,LeakyReLU,ReLU, AveragePooling2D\n",
    "from keras.optimizers import SGD, Adam,RMSprop,Adagrad,Adadelta\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_characters ={ 0:'R_train',1:'L_train',2:'F_train',3:'B_train'\n",
    "                }\n",
    "num_classes = len(map_characters)\n",
    "test_size = 0.1\n",
    "imgsPath = \"Lab1-1\"\n",
    "img_width = 32\n",
    "img_height = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pictures():\n",
    "    pics = []\n",
    "    labels = []\n",
    "    \n",
    "    for k, v in map_characters.items(): \n",
    "        \n",
    "        pictures = [k for k in glob.glob(imgsPath + \"/\" + v + \"/*\")]        \n",
    "        print(v + \" : \" + str(len(pictures))) \n",
    "        for i, pic in enumerate(pictures):\n",
    "            tmp_img = cv2.imread(pic)\n",
    "            tmp_img = cv2.cvtColor(tmp_img, cv2.COLOR_BGR2RGB) ##\n",
    "            tmp_img = cv2.resize(tmp_img, (img_height, img_width))          \n",
    "            pics.append(tmp_img)\n",
    "            labels.append(k)    \n",
    "    return np.array(pics), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(save=False, load=False):\n",
    "    if load: \n",
    "        \n",
    "        h5f = h5py.File('dataset.h5','r')\n",
    "        X_train = h5f['X_train'][:]\n",
    "        X_test = h5f['X_test'][:]\n",
    "        h5f.close()\n",
    "        \n",
    "        \n",
    "        h5f = h5py.File('labels.h5', 'r')\n",
    "        y_train = h5f['y_train'][:]\n",
    "        y_test = h5f['y_test'][:]\n",
    "        h5f.close()\n",
    "    else:\n",
    "        \n",
    "        X, y = load_pictures()\n",
    "        y = keras.utils.to_categorical(y, num_classes) \n",
    "        \n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size) \n",
    "        if save: \n",
    "            h5f = h5py.File('dataset.h5', 'w')\n",
    "            h5f.create_dataset('X_train', data=X_train)\n",
    "            h5f.create_dataset('X_test', data=X_test)\n",
    "            h5f.close()\n",
    "            \n",
    "            h5f = h5py.File('labels.h5', 'w')\n",
    "            h5f.create_dataset('y_train', data=y_train)\n",
    "            h5f.create_dataset('y_test', data=y_test)\n",
    "            h5f.close()\n",
    "    \n",
    "    \n",
    "    X_train = X_train.astype('float32') / 255.\n",
    "    X_test = X_test.astype('float32') / 255.\n",
    "    print(\"Train\", X_train.shape, y_train.shape)\n",
    "    print(\"Test\", X_test.shape, y_test.shape)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_train : 4865\n",
      "L_train : 4712\n",
      "F_train : 4877\n",
      "B_train : 5037\n",
      "Train (17541, 32, 32, 3) (17541, 4)\n",
      "Test (1950, 32, 32, 3) (1950, 4)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = get_dataset(save=False, load=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zedboard\n",
    "def create_model_face(input_shape):\n",
    "    kernel_size=3\n",
    "    model = Sequential([\n",
    "    Conv2D(32, (kernel_size, kernel_size), input_shape=input_shape, padding='same'),\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    #ReLU(),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    #Conv2D(32, (kernel_size, kernel_size), activation='relu', padding='same'),\n",
    "    #AveragePooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    #Dropout(0.25),\n",
    "    \n",
    "    Conv2D(64, (kernel_size, kernel_size), activation='relu', padding='same'),\n",
    "    AveragePooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "        \n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(4, activation='softmax')#有幾類?\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return model;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\Anaconda3\\envs\\test\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\user\\Anaconda3\\envs\\test\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\user\\Anaconda3\\envs\\test\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\user\\Anaconda3\\envs\\test\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\user\\Anaconda3\\envs\\test\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\user\\Anaconda3\\envs\\test\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\user\\Anaconda3\\envs\\test\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                262208    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 281,860\n",
      "Trainable params: 281,860\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model_face((img_height, img_width, 3)) ### 初始化一個模型\n",
    "model.summary() # 秀出模型架構"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\Anaconda3\\envs\\test\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\user\\Anaconda3\\envs\\test\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#original\n",
    "sgd = SGD(lr=0.03, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer=sgd,\n",
    "             metrics=['accuracy' ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15786, 32, 32, 3)\n",
      "(15786, 4)\n"
     ]
    }
   ],
   "source": [
    "valation_size = 0.1\n",
    "X_train2, X_val, y_train2, y_val = train_test_split(X_train, y_train, test_size=valation_size) \n",
    "print(X_train2.shape)\n",
    "print(y_train2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 15786 samples, validate on 1755 samples\n",
      "Epoch 1/500\n",
      "15786/15786 [==============================] - 10s 613us/step - loss: 1.2604 - acc: 0.4160 - val_loss: 1.0719 - val_acc: 0.5510\n",
      "Epoch 2/500\n",
      "15786/15786 [==============================] - 9s 600us/step - loss: 1.0836 - acc: 0.5398 - val_loss: 0.9132 - val_acc: 0.6285\n",
      "Epoch 3/500\n",
      "15786/15786 [==============================] - 9s 601us/step - loss: 0.9569 - acc: 0.6086 - val_loss: 0.8089 - val_acc: 0.6684\n",
      "Epoch 4/500\n",
      "15786/15786 [==============================] - 9s 600us/step - loss: 0.8613 - acc: 0.6523 - val_loss: 0.7514 - val_acc: 0.6900\n",
      "Epoch 5/500\n",
      "15786/15786 [==============================] - 9s 601us/step - loss: 0.7976 - acc: 0.6812 - val_loss: 0.6890 - val_acc: 0.7254\n",
      "Epoch 6/500\n",
      "15786/15786 [==============================] - 9s 600us/step - loss: 0.7484 - acc: 0.7018 - val_loss: 0.6340 - val_acc: 0.7464\n",
      "Epoch 7/500\n",
      "15786/15786 [==============================] - 10s 603us/step - loss: 0.7009 - acc: 0.7203 - val_loss: 0.6196 - val_acc: 0.7533\n",
      "Epoch 8/500\n",
      "15786/15786 [==============================] - 9s 601us/step - loss: 0.6820 - acc: 0.7331 - val_loss: 0.5918 - val_acc: 0.7647\n",
      "Epoch 9/500\n",
      "15786/15786 [==============================] - 9s 599us/step - loss: 0.6365 - acc: 0.7510 - val_loss: 0.5831 - val_acc: 0.7755\n",
      "Epoch 10/500\n",
      "15786/15786 [==============================] - 10s 603us/step - loss: 0.6194 - acc: 0.7567 - val_loss: 0.5718 - val_acc: 0.7772\n",
      "Epoch 11/500\n",
      "15786/15786 [==============================] - 9s 600us/step - loss: 0.5890 - acc: 0.7687 - val_loss: 0.5452 - val_acc: 0.7926\n",
      "Epoch 12/500\n",
      "15786/15786 [==============================] - 9s 601us/step - loss: 0.5691 - acc: 0.7769 - val_loss: 0.5302 - val_acc: 0.8063\n",
      "Epoch 13/500\n",
      "15786/15786 [==============================] - 10s 603us/step - loss: 0.5509 - acc: 0.7845 - val_loss: 0.5365 - val_acc: 0.8080\n",
      "Epoch 14/500\n",
      "15786/15786 [==============================] - 10s 606us/step - loss: 0.5313 - acc: 0.7865 - val_loss: 0.5281 - val_acc: 0.7972\n",
      "Epoch 15/500\n",
      "15786/15786 [==============================] - 9s 601us/step - loss: 0.5104 - acc: 0.8007 - val_loss: 0.5266 - val_acc: 0.8051\n",
      "Epoch 16/500\n",
      "15786/15786 [==============================] - 9s 599us/step - loss: 0.5064 - acc: 0.8018 - val_loss: 0.5047 - val_acc: 0.8114\n",
      "Epoch 17/500\n",
      "15786/15786 [==============================] - 10s 603us/step - loss: 0.4941 - acc: 0.8040 - val_loss: 0.4964 - val_acc: 0.8262\n",
      "Epoch 18/500\n",
      "15786/15786 [==============================] - 9s 602us/step - loss: 0.4788 - acc: 0.8120 - val_loss: 0.4731 - val_acc: 0.8291\n",
      "Epoch 19/500\n",
      "15786/15786 [==============================] - 9s 599us/step - loss: 0.4671 - acc: 0.8153 - val_loss: 0.4973 - val_acc: 0.8154\n",
      "Epoch 20/500\n",
      "15786/15786 [==============================] - 10s 604us/step - loss: 0.4602 - acc: 0.8149 - val_loss: 0.4865 - val_acc: 0.8222\n",
      "Epoch 21/500\n",
      "15786/15786 [==============================] - 9s 601us/step - loss: 0.4494 - acc: 0.8248 - val_loss: 0.4857 - val_acc: 0.8234\n",
      "Epoch 22/500\n",
      "15786/15786 [==============================] - 9s 602us/step - loss: 0.4493 - acc: 0.8235 - val_loss: 0.5069 - val_acc: 0.8188\n",
      "Epoch 23/500\n",
      "15786/15786 [==============================] - 9s 602us/step - loss: 0.4109 - acc: 0.8349 - val_loss: 0.4742 - val_acc: 0.8330\n",
      "Epoch 24/500\n",
      "15786/15786 [==============================] - 9s 601us/step - loss: 0.4224 - acc: 0.8309 - val_loss: 0.4981 - val_acc: 0.8194\n",
      "Epoch 25/500\n",
      "15786/15786 [==============================] - 10s 603us/step - loss: 0.4183 - acc: 0.8345 - val_loss: 0.5011 - val_acc: 0.8353\n",
      "Epoch 26/500\n",
      "15786/15786 [==============================] - 10s 602us/step - loss: 0.4041 - acc: 0.8397 - val_loss: 0.4660 - val_acc: 0.8382\n",
      "Epoch 27/500\n",
      "15786/15786 [==============================] - 10s 604us/step - loss: 0.3966 - acc: 0.8435 - val_loss: 0.4705 - val_acc: 0.8336\n",
      "Epoch 28/500\n",
      "15786/15786 [==============================] - 10s 621us/step - loss: 0.3855 - acc: 0.8492 - val_loss: 0.4744 - val_acc: 0.8444\n",
      "Epoch 29/500\n",
      "15786/15786 [==============================] - 10s 603us/step - loss: 0.3896 - acc: 0.8509 - val_loss: 0.5336 - val_acc: 0.8177\n",
      "Epoch 30/500\n",
      "15786/15786 [==============================] - 10s 604us/step - loss: 0.3945 - acc: 0.8449 - val_loss: 0.5020 - val_acc: 0.8256\n",
      "Epoch 31/500\n",
      "15786/15786 [==============================] - 10s 606us/step - loss: 0.3746 - acc: 0.8519 - val_loss: 0.4574 - val_acc: 0.8513\n",
      "Epoch 32/500\n",
      "15786/15786 [==============================] - 10s 615us/step - loss: 0.3817 - acc: 0.8525 - val_loss: 0.4568 - val_acc: 0.8479\n",
      "Epoch 33/500\n",
      "15786/15786 [==============================] - 10s 650us/step - loss: 0.3616 - acc: 0.8563 - val_loss: 0.4485 - val_acc: 0.8353\n",
      "Epoch 34/500\n",
      "15786/15786 [==============================] - 10s 647us/step - loss: 0.3634 - acc: 0.8572 - val_loss: 0.4545 - val_acc: 0.8376\n",
      "Epoch 35/500\n",
      "15786/15786 [==============================] - 10s 614us/step - loss: 0.3608 - acc: 0.8578 - val_loss: 0.4610 - val_acc: 0.8439\n",
      "Epoch 36/500\n",
      "15786/15786 [==============================] - 10s 602us/step - loss: 0.3558 - acc: 0.8593 - val_loss: 0.4711 - val_acc: 0.8456\n",
      "Epoch 37/500\n",
      "15786/15786 [==============================] - 10s 605us/step - loss: 0.3537 - acc: 0.8651 - val_loss: 0.5204 - val_acc: 0.8359\n",
      "Epoch 38/500\n",
      "15786/15786 [==============================] - 10s 604us/step - loss: 0.3444 - acc: 0.8650 - val_loss: 0.4842 - val_acc: 0.8479\n",
      "Epoch 39/500\n",
      "15786/15786 [==============================] - 9s 601us/step - loss: 0.3390 - acc: 0.8694 - val_loss: 0.4617 - val_acc: 0.8547\n",
      "Epoch 40/500\n",
      "15786/15786 [==============================] - 10s 604us/step - loss: 0.3490 - acc: 0.8613 - val_loss: 0.4493 - val_acc: 0.8462\n",
      "Epoch 41/500\n",
      "15786/15786 [==============================] - 9s 601us/step - loss: 0.3412 - acc: 0.8648 - val_loss: 0.5033 - val_acc: 0.8313\n",
      "Epoch 42/500\n",
      "15786/15786 [==============================] - 10s 602us/step - loss: 0.3433 - acc: 0.8661 - val_loss: 0.5110 - val_acc: 0.8342\n",
      "Epoch 43/500\n",
      "15786/15786 [==============================] - 10s 602us/step - loss: 0.3286 - acc: 0.8677 - val_loss: 0.4634 - val_acc: 0.8467\n",
      "Epoch 44/500\n",
      "15786/15786 [==============================] - 9s 601us/step - loss: 0.3440 - acc: 0.8665 - val_loss: 0.4658 - val_acc: 0.8450\n",
      "Epoch 45/500\n",
      "15786/15786 [==============================] - 10s 606us/step - loss: 0.3389 - acc: 0.8687 - val_loss: 0.5060 - val_acc: 0.8479\n",
      "Epoch 46/500\n",
      "15786/15786 [==============================] - 9s 602us/step - loss: 0.3251 - acc: 0.8699 - val_loss: 0.4562 - val_acc: 0.8484\n",
      "Epoch 47/500\n",
      "15786/15786 [==============================] - 10s 604us/step - loss: 0.3256 - acc: 0.8744 - val_loss: 0.4603 - val_acc: 0.8519\n",
      "Epoch 48/500\n",
      "15786/15786 [==============================] - 10s 604us/step - loss: 0.3128 - acc: 0.8763 - val_loss: 0.4319 - val_acc: 0.8558\n",
      "Epoch 49/500\n",
      "15786/15786 [==============================] - 9s 601us/step - loss: 0.3171 - acc: 0.8773 - val_loss: 0.4491 - val_acc: 0.8621\n",
      "Epoch 50/500\n",
      "15786/15786 [==============================] - 10s 603us/step - loss: 0.3278 - acc: 0.8752 - val_loss: 0.5190 - val_acc: 0.8427\n",
      "Epoch 51/500\n",
      "15786/15786 [==============================] - 9s 600us/step - loss: 0.3182 - acc: 0.8726 - val_loss: 0.4288 - val_acc: 0.8610\n",
      "Epoch 52/500\n",
      "15786/15786 [==============================] - 10s 604us/step - loss: 0.3108 - acc: 0.8807 - val_loss: 0.5189 - val_acc: 0.8473\n",
      "Epoch 53/500\n",
      "15786/15786 [==============================] - 9s 601us/step - loss: 0.3119 - acc: 0.8822 - val_loss: 0.4997 - val_acc: 0.8444\n",
      "Epoch 54/500\n",
      "15786/15786 [==============================] - 9s 600us/step - loss: 0.3025 - acc: 0.8796 - val_loss: 0.4557 - val_acc: 0.8638\n",
      "Epoch 55/500\n",
      "15786/15786 [==============================] - 10s 602us/step - loss: 0.3076 - acc: 0.8796 - val_loss: 0.5517 - val_acc: 0.8433\n",
      "Epoch 56/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15786/15786 [==============================] - 9s 599us/step - loss: 0.3058 - acc: 0.8808 - val_loss: 0.4768 - val_acc: 0.8581\n",
      "Epoch 57/500\n",
      "15786/15786 [==============================] - 10s 603us/step - loss: 0.3100 - acc: 0.8808 - val_loss: 0.5208 - val_acc: 0.8462\n",
      "Epoch 58/500\n",
      "15786/15786 [==============================] - 9s 600us/step - loss: 0.3098 - acc: 0.8784 - val_loss: 0.4899 - val_acc: 0.8479\n",
      "Epoch 59/500\n",
      "15786/15786 [==============================] - 9s 601us/step - loss: 0.2970 - acc: 0.8865 - val_loss: 0.4548 - val_acc: 0.8621\n",
      "Epoch 60/500\n",
      "15786/15786 [==============================] - 10s 603us/step - loss: 0.2861 - acc: 0.8860 - val_loss: 0.4848 - val_acc: 0.8632\n",
      "Epoch 61/500\n",
      "15786/15786 [==============================] - 9s 600us/step - loss: 0.2940 - acc: 0.8859 - val_loss: 0.4694 - val_acc: 0.8627\n",
      "Epoch 62/500\n",
      "15786/15786 [==============================] - 10s 610us/step - loss: 0.2978 - acc: 0.8830 - val_loss: 0.4798 - val_acc: 0.8541\n",
      "Epoch 63/500\n",
      "15786/15786 [==============================] - 9s 602us/step - loss: 0.2908 - acc: 0.8879 - val_loss: 0.5174 - val_acc: 0.8553\n",
      "Epoch 64/500\n",
      "15786/15786 [==============================] - 10s 603us/step - loss: 0.2930 - acc: 0.8904 - val_loss: 0.4736 - val_acc: 0.8564\n",
      "Epoch 65/500\n",
      "15786/15786 [==============================] - 10s 603us/step - loss: 0.2929 - acc: 0.8869 - val_loss: 0.4858 - val_acc: 0.8490\n",
      "Epoch 66/500\n",
      "15786/15786 [==============================] - 9s 600us/step - loss: 0.2891 - acc: 0.8884 - val_loss: 0.4652 - val_acc: 0.8610\n",
      "Epoch 67/500\n",
      "15786/15786 [==============================] - 10s 603us/step - loss: 0.2970 - acc: 0.8838 - val_loss: 0.4827 - val_acc: 0.8553\n",
      "Epoch 68/500\n",
      "15786/15786 [==============================] - 10s 602us/step - loss: 0.2821 - acc: 0.8922 - val_loss: 0.4685 - val_acc: 0.8593\n",
      "Epoch 69/500\n",
      "15786/15786 [==============================] - 10s 604us/step - loss: 0.2853 - acc: 0.8911 - val_loss: 0.4592 - val_acc: 0.8530\n",
      "Epoch 70/500\n",
      "15786/15786 [==============================] - 10s 603us/step - loss: 0.2912 - acc: 0.8881 - val_loss: 0.4648 - val_acc: 0.8570\n",
      "Epoch 71/500\n",
      "15786/15786 [==============================] - 9s 600us/step - loss: 0.2754 - acc: 0.8902 - val_loss: 0.4319 - val_acc: 0.8672\n",
      "Epoch 72/500\n",
      "15786/15786 [==============================] - 10s 604us/step - loss: 0.2824 - acc: 0.8933 - val_loss: 0.4952 - val_acc: 0.8439\n",
      "Epoch 73/500\n",
      "15786/15786 [==============================] - 10s 605us/step - loss: 0.2759 - acc: 0.8910 - val_loss: 0.4814 - val_acc: 0.8564\n",
      "Epoch 74/500\n",
      "15786/15786 [==============================] - 10s 606us/step - loss: 0.2853 - acc: 0.8899 - val_loss: 0.4619 - val_acc: 0.8575\n",
      "Epoch 75/500\n",
      "15786/15786 [==============================] - 10s 604us/step - loss: 0.2893 - acc: 0.8887 - val_loss: 0.4737 - val_acc: 0.8490\n",
      "Epoch 76/500\n",
      "15786/15786 [==============================] - 9s 602us/step - loss: 0.2646 - acc: 0.8972 - val_loss: 0.4496 - val_acc: 0.8695\n",
      "Epoch 77/500\n",
      "15786/15786 [==============================] - 10s 605us/step - loss: 0.2857 - acc: 0.8924 - val_loss: 0.4951 - val_acc: 0.8524\n",
      "Epoch 78/500\n",
      "15786/15786 [==============================] - 9s 599us/step - loss: 0.2741 - acc: 0.8955 - val_loss: 0.4990 - val_acc: 0.8547\n",
      "Epoch 79/500\n",
      "15786/15786 [==============================] - 10s 603us/step - loss: 0.2801 - acc: 0.8948 - val_loss: 0.4798 - val_acc: 0.8672\n",
      "Epoch 80/500\n",
      "15786/15786 [==============================] - 10s 604us/step - loss: 0.2713 - acc: 0.8950 - val_loss: 0.5229 - val_acc: 0.8604\n",
      "Epoch 81/500\n",
      "15786/15786 [==============================] - 10s 602us/step - loss: 0.2723 - acc: 0.8948 - val_loss: 0.4761 - val_acc: 0.8621\n",
      "Epoch 82/500\n",
      "15786/15786 [==============================] - 10s 604us/step - loss: 0.2837 - acc: 0.8915 - val_loss: 0.4650 - val_acc: 0.8632\n",
      "Epoch 83/500\n",
      "15786/15786 [==============================] - 9s 600us/step - loss: 0.2602 - acc: 0.9012 - val_loss: 0.4811 - val_acc: 0.8547\n",
      "Epoch 84/500\n",
      "15786/15786 [==============================] - 10s 602us/step - loss: 0.2654 - acc: 0.8979 - val_loss: 0.4768 - val_acc: 0.8575\n",
      "Epoch 85/500\n",
      "15786/15786 [==============================] - 10s 603us/step - loss: 0.2693 - acc: 0.8954 - val_loss: 0.4801 - val_acc: 0.8593\n",
      "Epoch 86/500\n",
      "15786/15786 [==============================] - 9s 601us/step - loss: 0.2598 - acc: 0.8997 - val_loss: 0.4317 - val_acc: 0.8661\n",
      "Epoch 87/500\n",
      "15786/15786 [==============================] - 10s 604us/step - loss: 0.2700 - acc: 0.8954 - val_loss: 0.4996 - val_acc: 0.8547\n",
      "Epoch 88/500\n",
      "15786/15786 [==============================] - 9s 601us/step - loss: 0.2649 - acc: 0.8975 - val_loss: 0.4781 - val_acc: 0.8632\n",
      "Epoch 89/500\n",
      "15786/15786 [==============================] - 10s 604us/step - loss: 0.2639 - acc: 0.8982 - val_loss: 0.4786 - val_acc: 0.8718\n",
      "Epoch 90/500\n",
      "15786/15786 [==============================] - 10s 604us/step - loss: 0.2585 - acc: 0.9009 - val_loss: 0.4806 - val_acc: 0.8598\n",
      "Epoch 91/500\n",
      "15786/15786 [==============================] - 9s 601us/step - loss: 0.2594 - acc: 0.8988 - val_loss: 0.4587 - val_acc: 0.8764\n",
      "Epoch 92/500\n",
      "15786/15786 [==============================] - 10s 602us/step - loss: 0.2697 - acc: 0.8969 - val_loss: 0.4572 - val_acc: 0.8712\n",
      "Epoch 93/500\n",
      "15786/15786 [==============================] - 10s 603us/step - loss: 0.2591 - acc: 0.9021 - val_loss: 0.4482 - val_acc: 0.8661\n",
      "Epoch 94/500\n",
      "15786/15786 [==============================] - 10s 605us/step - loss: 0.2621 - acc: 0.8982 - val_loss: 0.4947 - val_acc: 0.8632\n",
      "Epoch 95/500\n",
      "15786/15786 [==============================] - 10s 603us/step - loss: 0.2513 - acc: 0.9039 - val_loss: 0.4944 - val_acc: 0.8655\n",
      "Epoch 96/500\n",
      "15786/15786 [==============================] - 10s 602us/step - loss: 0.2640 - acc: 0.8997 - val_loss: 0.4387 - val_acc: 0.8741\n",
      "Epoch 97/500\n",
      "15786/15786 [==============================] - 10s 602us/step - loss: 0.2541 - acc: 0.9000 - val_loss: 0.4392 - val_acc: 0.8752\n",
      "Epoch 98/500\n",
      "15786/15786 [==============================] - 9s 602us/step - loss: 0.2612 - acc: 0.9016 - val_loss: 0.5043 - val_acc: 0.8632\n",
      "Epoch 99/500\n",
      "15786/15786 [==============================] - 10s 604us/step - loss: 0.2561 - acc: 0.9050 - val_loss: 0.4738 - val_acc: 0.8667\n",
      "Epoch 100/500\n",
      "15786/15786 [==============================] - 9s 601us/step - loss: 0.2713 - acc: 0.9004 - val_loss: 0.4662 - val_acc: 0.8672\n",
      "Epoch 101/500\n",
      "15786/15786 [==============================] - 10s 604us/step - loss: 0.2531 - acc: 0.9049 - val_loss: 0.5164 - val_acc: 0.8678\n",
      "Epoch 102/500\n",
      "15786/15786 [==============================] - 10s 603us/step - loss: 0.2449 - acc: 0.9057 - val_loss: 0.4946 - val_acc: 0.8655\n",
      "Epoch 103/500\n",
      "15786/15786 [==============================] - 10s 602us/step - loss: 0.2518 - acc: 0.9033 - val_loss: 0.4752 - val_acc: 0.8741\n",
      "Epoch 104/500\n",
      "15786/15786 [==============================] - 10s 604us/step - loss: 0.2618 - acc: 0.9013 - val_loss: 0.4832 - val_acc: 0.8604\n",
      "Epoch 105/500\n",
      "15786/15786 [==============================] - 10s 603us/step - loss: 0.2603 - acc: 0.9015 - val_loss: 0.4972 - val_acc: 0.8678\n",
      "Epoch 106/500\n",
      "15786/15786 [==============================] - 10s 602us/step - loss: 0.2536 - acc: 0.9023 - val_loss: 0.5044 - val_acc: 0.8650\n",
      "Epoch 107/500\n",
      "15786/15786 [==============================] - 10s 603us/step - loss: 0.2538 - acc: 0.9032 - val_loss: 0.4703 - val_acc: 0.8701\n",
      "Epoch 108/500\n",
      "15786/15786 [==============================] - 10s 603us/step - loss: 0.2397 - acc: 0.9078 - val_loss: 0.4722 - val_acc: 0.8701\n",
      "Epoch 109/500\n",
      "15786/15786 [==============================] - 10s 603us/step - loss: 0.2578 - acc: 0.9009 - val_loss: 0.4649 - val_acc: 0.8678\n",
      "Epoch 110/500\n",
      "15786/15786 [==============================] - 9s 601us/step - loss: 0.2585 - acc: 0.9022 - val_loss: 0.4876 - val_acc: 0.8661\n",
      "Epoch 111/500\n",
      "15786/15786 [==============================] - 10s 602us/step - loss: 0.2410 - acc: 0.9073 - val_loss: 0.5156 - val_acc: 0.8593\n",
      "Epoch 112/500\n",
      "15786/15786 [==============================] - 10s 603us/step - loss: 0.2453 - acc: 0.9061 - val_loss: 0.4878 - val_acc: 0.8678\n",
      "Epoch 113/500\n",
      "15786/15786 [==============================] - 9s 602us/step - loss: 0.2446 - acc: 0.9085 - val_loss: 0.5086 - val_acc: 0.8678\n",
      "Epoch 114/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15786/15786 [==============================] - 10s 604us/step - loss: 0.2393 - acc: 0.9078 - val_loss: 0.5003 - val_acc: 0.8632\n",
      "Epoch 115/500\n",
      "15786/15786 [==============================] - 9s 601us/step - loss: 0.2474 - acc: 0.9080 - val_loss: 0.4872 - val_acc: 0.8741\n",
      "Epoch 116/500\n",
      "15786/15786 [==============================] - 9s 601us/step - loss: 0.2442 - acc: 0.9076 - val_loss: 0.4746 - val_acc: 0.8712\n",
      "Epoch 117/500\n",
      "15786/15786 [==============================] - 9s 600us/step - loss: 0.2621 - acc: 0.8997 - val_loss: 0.4924 - val_acc: 0.8650\n",
      "Epoch 118/500\n",
      "15786/15786 [==============================] - 9s 598us/step - loss: 0.2458 - acc: 0.9071 - val_loss: 0.4254 - val_acc: 0.8741\n",
      "Epoch 119/500\n",
      "15786/15786 [==============================] - 9s 601us/step - loss: 0.2526 - acc: 0.9054 - val_loss: 0.4827 - val_acc: 0.8655\n",
      "Epoch 120/500\n",
      "15786/15786 [==============================] - 9s 600us/step - loss: 0.2448 - acc: 0.9074 - val_loss: 0.4826 - val_acc: 0.8707\n",
      "Epoch 121/500\n",
      "15786/15786 [==============================] - 10s 605us/step - loss: 0.2331 - acc: 0.9124 - val_loss: 0.4955 - val_acc: 0.8593\n",
      "Epoch 122/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.2448 - acc: 0.9109 - val_loss: 0.4842 - val_acc: 0.8672\n",
      "Epoch 123/500\n",
      "15786/15786 [==============================] - 9s 602us/step - loss: 0.2427 - acc: 0.9109 - val_loss: 0.4730 - val_acc: 0.8667\n",
      "Epoch 124/500\n",
      "15786/15786 [==============================] - 10s 605us/step - loss: 0.2413 - acc: 0.9088 - val_loss: 0.4865 - val_acc: 0.8781\n",
      "Epoch 125/500\n",
      "15786/15786 [==============================] - 9s 601us/step - loss: 0.2360 - acc: 0.9147 - val_loss: 0.4833 - val_acc: 0.8729\n",
      "Epoch 126/500\n",
      "15786/15786 [==============================] - 10s 604us/step - loss: 0.2242 - acc: 0.9135 - val_loss: 0.5119 - val_acc: 0.8667\n",
      "Epoch 127/500\n",
      "15786/15786 [==============================] - 10s 603us/step - loss: 0.2366 - acc: 0.9115 - val_loss: 0.4594 - val_acc: 0.8764\n",
      "Epoch 128/500\n",
      "15786/15786 [==============================] - 9s 601us/step - loss: 0.2418 - acc: 0.9102 - val_loss: 0.4760 - val_acc: 0.8558\n",
      "Epoch 129/500\n",
      "15786/15786 [==============================] - 10s 604us/step - loss: 0.2364 - acc: 0.9132 - val_loss: 0.4831 - val_acc: 0.8724\n",
      "Epoch 130/500\n",
      "15786/15786 [==============================] - 9s 600us/step - loss: 0.2286 - acc: 0.9143 - val_loss: 0.4564 - val_acc: 0.8695\n",
      "Epoch 131/500\n",
      "15786/15786 [==============================] - 10s 604us/step - loss: 0.2348 - acc: 0.9104 - val_loss: 0.4322 - val_acc: 0.8786\n",
      "Epoch 132/500\n",
      "15786/15786 [==============================] - 10s 603us/step - loss: 0.2344 - acc: 0.9126 - val_loss: 0.4767 - val_acc: 0.8672\n",
      "Epoch 133/500\n",
      "15786/15786 [==============================] - 9s 602us/step - loss: 0.2279 - acc: 0.9145 - val_loss: 0.4957 - val_acc: 0.8786\n",
      "Epoch 134/500\n",
      "15786/15786 [==============================] - 10s 605us/step - loss: 0.2273 - acc: 0.9157 - val_loss: 0.4770 - val_acc: 0.8764\n",
      "Epoch 135/500\n",
      "15786/15786 [==============================] - 9s 601us/step - loss: 0.2153 - acc: 0.9179 - val_loss: 0.5107 - val_acc: 0.8718\n",
      "Epoch 136/500\n",
      "15786/15786 [==============================] - 10s 604us/step - loss: 0.2367 - acc: 0.9107 - val_loss: 0.4615 - val_acc: 0.8672\n",
      "Epoch 137/500\n",
      "15786/15786 [==============================] - 10s 602us/step - loss: 0.2417 - acc: 0.9069 - val_loss: 0.4919 - val_acc: 0.8695\n",
      "Epoch 138/500\n",
      "15786/15786 [==============================] - 10s 604us/step - loss: 0.2335 - acc: 0.9114 - val_loss: 0.4793 - val_acc: 0.8781\n",
      "Epoch 139/500\n",
      "15786/15786 [==============================] - 10s 605us/step - loss: 0.2287 - acc: 0.9150 - val_loss: 0.5125 - val_acc: 0.8650\n",
      "Epoch 140/500\n",
      "15786/15786 [==============================] - 10s 605us/step - loss: 0.2344 - acc: 0.9120 - val_loss: 0.4496 - val_acc: 0.8695\n",
      "Epoch 141/500\n",
      "15786/15786 [==============================] - 10s 604us/step - loss: 0.2224 - acc: 0.9161 - val_loss: 0.4880 - val_acc: 0.8655\n",
      "Epoch 142/500\n",
      "15786/15786 [==============================] - 10s 602us/step - loss: 0.2220 - acc: 0.9172 - val_loss: 0.4590 - val_acc: 0.8752\n",
      "Epoch 143/500\n",
      "15786/15786 [==============================] - 10s 606us/step - loss: 0.2295 - acc: 0.9126 - val_loss: 0.4870 - val_acc: 0.8718\n",
      "Epoch 144/500\n",
      "15786/15786 [==============================] - 10s 603us/step - loss: 0.2386 - acc: 0.9102 - val_loss: 0.4681 - val_acc: 0.8758\n",
      "Epoch 145/500\n",
      "15786/15786 [==============================] - 10s 602us/step - loss: 0.2387 - acc: 0.9125 - val_loss: 0.5047 - val_acc: 0.8689\n",
      "Epoch 146/500\n",
      "15786/15786 [==============================] - 10s 605us/step - loss: 0.2328 - acc: 0.9117 - val_loss: 0.5142 - val_acc: 0.8684\n",
      "Epoch 147/500\n",
      "15786/15786 [==============================] - 10s 602us/step - loss: 0.2367 - acc: 0.9114 - val_loss: 0.5043 - val_acc: 0.8701\n",
      "Epoch 148/500\n",
      "15786/15786 [==============================] - 10s 603us/step - loss: 0.2266 - acc: 0.9114 - val_loss: 0.5046 - val_acc: 0.8718\n",
      "Epoch 149/500\n",
      "15786/15786 [==============================] - 10s 603us/step - loss: 0.2209 - acc: 0.9159 - val_loss: 0.5143 - val_acc: 0.8741\n",
      "Epoch 150/500\n",
      "15786/15786 [==============================] - 10s 602us/step - loss: 0.2282 - acc: 0.9146 - val_loss: 0.5116 - val_acc: 0.8724\n",
      "Epoch 151/500\n",
      "15786/15786 [==============================] - 10s 605us/step - loss: 0.2257 - acc: 0.9157 - val_loss: 0.4971 - val_acc: 0.8758\n",
      "Epoch 152/500\n",
      "15786/15786 [==============================] - 9s 601us/step - loss: 0.2277 - acc: 0.9150 - val_loss: 0.5049 - val_acc: 0.8809\n",
      "Epoch 153/500\n",
      "15786/15786 [==============================] - 10s 605us/step - loss: 0.2193 - acc: 0.9175 - val_loss: 0.4858 - val_acc: 0.8712\n",
      "Epoch 154/500\n",
      "15786/15786 [==============================] - 10s 605us/step - loss: 0.2161 - acc: 0.9202 - val_loss: 0.5280 - val_acc: 0.8638\n",
      "Epoch 155/500\n",
      "15786/15786 [==============================] - 10s 605us/step - loss: 0.2294 - acc: 0.9159 - val_loss: 0.5875 - val_acc: 0.8627\n",
      "Epoch 156/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.2353 - acc: 0.9156 - val_loss: 0.4551 - val_acc: 0.8729\n",
      "Epoch 157/500\n",
      "15786/15786 [==============================] - 10s 606us/step - loss: 0.2099 - acc: 0.9221 - val_loss: 0.4623 - val_acc: 0.8809\n",
      "Epoch 158/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.2261 - acc: 0.9169 - val_loss: 0.5056 - val_acc: 0.8707\n",
      "Epoch 159/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.2264 - acc: 0.9155 - val_loss: 0.4164 - val_acc: 0.8866\n",
      "Epoch 160/500\n",
      "15786/15786 [==============================] - 10s 604us/step - loss: 0.2252 - acc: 0.9142 - val_loss: 0.4801 - val_acc: 0.8798\n",
      "Epoch 161/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.2377 - acc: 0.9136 - val_loss: 0.5379 - val_acc: 0.8752\n",
      "Epoch 162/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.2218 - acc: 0.9174 - val_loss: 0.4968 - val_acc: 0.8655\n",
      "Epoch 163/500\n",
      "15786/15786 [==============================] - 10s 610us/step - loss: 0.2190 - acc: 0.9190 - val_loss: 0.5032 - val_acc: 0.8832\n",
      "Epoch 164/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.2423 - acc: 0.9091 - val_loss: 0.4614 - val_acc: 0.8746\n",
      "Epoch 165/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.2182 - acc: 0.9187 - val_loss: 0.4862 - val_acc: 0.8803\n",
      "Epoch 166/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.2301 - acc: 0.9157 - val_loss: 0.4809 - val_acc: 0.8718\n",
      "Epoch 167/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.2196 - acc: 0.9197 - val_loss: 0.4800 - val_acc: 0.8821\n",
      "Epoch 168/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.2084 - acc: 0.9237 - val_loss: 0.5216 - val_acc: 0.8758\n",
      "Epoch 169/500\n",
      "15786/15786 [==============================] - 10s 610us/step - loss: 0.2130 - acc: 0.9193 - val_loss: 0.4914 - val_acc: 0.8707\n",
      "Epoch 170/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.2149 - acc: 0.9211 - val_loss: 0.4975 - val_acc: 0.8695\n",
      "Epoch 171/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.2125 - acc: 0.9195 - val_loss: 0.4781 - val_acc: 0.8792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/500\n",
      "15786/15786 [==============================] - 10s 603us/step - loss: 0.2219 - acc: 0.9172 - val_loss: 0.4931 - val_acc: 0.8746\n",
      "Epoch 173/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.2142 - acc: 0.9201 - val_loss: 0.6067 - val_acc: 0.8718\n",
      "Epoch 174/500\n",
      "15786/15786 [==============================] - 10s 610us/step - loss: 0.2168 - acc: 0.9200 - val_loss: 0.4675 - val_acc: 0.8684\n",
      "Epoch 175/500\n",
      "15786/15786 [==============================] - 10s 606us/step - loss: 0.2214 - acc: 0.9149 - val_loss: 0.5620 - val_acc: 0.8530\n",
      "Epoch 176/500\n",
      "15786/15786 [==============================] - 10s 610us/step - loss: 0.2265 - acc: 0.9155 - val_loss: 0.5316 - val_acc: 0.8746\n",
      "Epoch 177/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.2261 - acc: 0.9171 - val_loss: 0.4903 - val_acc: 0.8655\n",
      "Epoch 178/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.2116 - acc: 0.9204 - val_loss: 0.5172 - val_acc: 0.8735\n",
      "Epoch 179/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.2264 - acc: 0.9142 - val_loss: 0.5431 - val_acc: 0.8712\n",
      "Epoch 180/500\n",
      "15786/15786 [==============================] - 10s 606us/step - loss: 0.2118 - acc: 0.9217 - val_loss: 0.5146 - val_acc: 0.8724\n",
      "Epoch 181/500\n",
      "15786/15786 [==============================] - 10s 611us/step - loss: 0.2373 - acc: 0.9145 - val_loss: 0.4949 - val_acc: 0.8718\n",
      "Epoch 182/500\n",
      "15786/15786 [==============================] - 10s 605us/step - loss: 0.2093 - acc: 0.9201 - val_loss: 0.5337 - val_acc: 0.8832\n",
      "Epoch 183/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.2138 - acc: 0.9176 - val_loss: 0.5292 - val_acc: 0.8667\n",
      "Epoch 184/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.2111 - acc: 0.9199 - val_loss: 0.5019 - val_acc: 0.8650\n",
      "Epoch 185/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.2163 - acc: 0.9197 - val_loss: 0.5724 - val_acc: 0.8707\n",
      "Epoch 186/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.2102 - acc: 0.9189 - val_loss: 0.5807 - val_acc: 0.8644\n",
      "Epoch 187/500\n",
      "15786/15786 [==============================] - 10s 606us/step - loss: 0.2105 - acc: 0.9236 - val_loss: 0.5354 - val_acc: 0.8678\n",
      "Epoch 188/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.2194 - acc: 0.9191 - val_loss: 0.5601 - val_acc: 0.8570\n",
      "Epoch 189/500\n",
      "15786/15786 [==============================] - 10s 610us/step - loss: 0.2106 - acc: 0.9228 - val_loss: 0.5112 - val_acc: 0.8667\n",
      "Epoch 190/500\n",
      "15786/15786 [==============================] - 10s 606us/step - loss: 0.2181 - acc: 0.9204 - val_loss: 0.5259 - val_acc: 0.8775\n",
      "Epoch 191/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.2119 - acc: 0.9208 - val_loss: 0.5321 - val_acc: 0.8741\n",
      "Epoch 192/500\n",
      "15786/15786 [==============================] - 10s 605us/step - loss: 0.2153 - acc: 0.9212 - val_loss: 0.5395 - val_acc: 0.8678\n",
      "Epoch 193/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.2070 - acc: 0.9228 - val_loss: 0.5076 - val_acc: 0.8821\n",
      "Epoch 194/500\n",
      "15786/15786 [==============================] - 10s 610us/step - loss: 0.2073 - acc: 0.9235 - val_loss: 0.5633 - val_acc: 0.8724\n",
      "Epoch 195/500\n",
      "15786/15786 [==============================] - 10s 605us/step - loss: 0.2075 - acc: 0.9228 - val_loss: 0.5054 - val_acc: 0.8803\n",
      "Epoch 196/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.2108 - acc: 0.9228 - val_loss: 0.4915 - val_acc: 0.8815\n",
      "Epoch 197/500\n",
      "15786/15786 [==============================] - 10s 605us/step - loss: 0.2082 - acc: 0.9224 - val_loss: 0.5446 - val_acc: 0.8741\n",
      "Epoch 198/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.2184 - acc: 0.9185 - val_loss: 0.5056 - val_acc: 0.8798\n",
      "Epoch 199/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.2088 - acc: 0.9211 - val_loss: 0.5522 - val_acc: 0.8655\n",
      "Epoch 200/500\n",
      "15786/15786 [==============================] - 10s 606us/step - loss: 0.2124 - acc: 0.9211 - val_loss: 0.5146 - val_acc: 0.8718\n",
      "Epoch 201/500\n",
      "15786/15786 [==============================] - 10s 610us/step - loss: 0.2133 - acc: 0.9194 - val_loss: 0.5095 - val_acc: 0.8695\n",
      "Epoch 202/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.2203 - acc: 0.9209 - val_loss: 0.5593 - val_acc: 0.8764\n",
      "Epoch 203/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.2071 - acc: 0.9221 - val_loss: 0.5173 - val_acc: 0.8809\n",
      "Epoch 204/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.2092 - acc: 0.9244 - val_loss: 0.5101 - val_acc: 0.8764\n",
      "Epoch 205/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.2135 - acc: 0.9190 - val_loss: 0.5394 - val_acc: 0.8746\n",
      "Epoch 206/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.2193 - acc: 0.9206 - val_loss: 0.4352 - val_acc: 0.8764\n",
      "Epoch 207/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.2132 - acc: 0.9225 - val_loss: 0.5006 - val_acc: 0.8792\n",
      "Epoch 208/500\n",
      "15786/15786 [==============================] - 10s 610us/step - loss: 0.2167 - acc: 0.9205 - val_loss: 0.5308 - val_acc: 0.8764\n",
      "Epoch 209/500\n",
      "15786/15786 [==============================] - 10s 610us/step - loss: 0.2179 - acc: 0.9195 - val_loss: 0.4803 - val_acc: 0.8735\n",
      "Epoch 210/500\n",
      "15786/15786 [==============================] - 10s 606us/step - loss: 0.2164 - acc: 0.9208 - val_loss: 0.4756 - val_acc: 0.8860\n",
      "Epoch 211/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.2093 - acc: 0.9215 - val_loss: 0.4957 - val_acc: 0.8752\n",
      "Epoch 212/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.2057 - acc: 0.9253 - val_loss: 0.5120 - val_acc: 0.8786\n",
      "Epoch 213/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.2167 - acc: 0.9213 - val_loss: 0.5110 - val_acc: 0.8849\n",
      "Epoch 214/500\n",
      "15786/15786 [==============================] - 10s 610us/step - loss: 0.2072 - acc: 0.9235 - val_loss: 0.5286 - val_acc: 0.8741\n",
      "Epoch 215/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.2021 - acc: 0.9275 - val_loss: 0.4961 - val_acc: 0.8672\n",
      "Epoch 216/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1985 - acc: 0.9261 - val_loss: 0.5081 - val_acc: 0.8849\n",
      "Epoch 217/500\n",
      "15786/15786 [==============================] - 10s 605us/step - loss: 0.2037 - acc: 0.9265 - val_loss: 0.5058 - val_acc: 0.8775\n",
      "Epoch 218/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.2140 - acc: 0.9215 - val_loss: 0.4476 - val_acc: 0.8917\n",
      "Epoch 219/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.2019 - acc: 0.9269 - val_loss: 0.4538 - val_acc: 0.8832\n",
      "Epoch 220/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.2064 - acc: 0.9216 - val_loss: 0.4968 - val_acc: 0.8838\n",
      "Epoch 221/500\n",
      "15786/15786 [==============================] - 10s 610us/step - loss: 0.1951 - acc: 0.9284 - val_loss: 0.4828 - val_acc: 0.8650\n",
      "Epoch 222/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.2134 - acc: 0.9251 - val_loss: 0.4932 - val_acc: 0.8792\n",
      "Epoch 223/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.2065 - acc: 0.9256 - val_loss: 0.4783 - val_acc: 0.8906\n",
      "Epoch 224/500\n",
      "15786/15786 [==============================] - 10s 610us/step - loss: 0.2095 - acc: 0.9210 - val_loss: 0.4985 - val_acc: 0.8729\n",
      "Epoch 225/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.2016 - acc: 0.9248 - val_loss: 0.5197 - val_acc: 0.8866\n",
      "Epoch 226/500\n",
      "15786/15786 [==============================] - 10s 611us/step - loss: 0.2028 - acc: 0.9279 - val_loss: 0.5403 - val_acc: 0.8764\n",
      "Epoch 227/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.2050 - acc: 0.9237 - val_loss: 0.5462 - val_acc: 0.8644\n",
      "Epoch 228/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.2161 - acc: 0.9230 - val_loss: 0.5302 - val_acc: 0.8707\n",
      "Epoch 229/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.2024 - acc: 0.9251 - val_loss: 0.5443 - val_acc: 0.8826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 230/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.1999 - acc: 0.9284 - val_loss: 0.5004 - val_acc: 0.8798\n",
      "Epoch 231/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.2159 - acc: 0.9237 - val_loss: 0.5145 - val_acc: 0.8775\n",
      "Epoch 232/500\n",
      "15786/15786 [==============================] - 10s 606us/step - loss: 0.2166 - acc: 0.9204 - val_loss: 0.4885 - val_acc: 0.8906\n",
      "Epoch 233/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.2052 - acc: 0.9261 - val_loss: 0.5459 - val_acc: 0.8826\n",
      "Epoch 234/500\n",
      "15786/15786 [==============================] - 10s 610us/step - loss: 0.2014 - acc: 0.9261 - val_loss: 0.4905 - val_acc: 0.8741\n",
      "Epoch 235/500\n",
      "15786/15786 [==============================] - 10s 606us/step - loss: 0.1936 - acc: 0.9294 - val_loss: 0.5420 - val_acc: 0.8769\n",
      "Epoch 236/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.2109 - acc: 0.9261 - val_loss: 0.5052 - val_acc: 0.8815\n",
      "Epoch 237/500\n",
      "15786/15786 [==============================] - 10s 606us/step - loss: 0.2006 - acc: 0.9255 - val_loss: 0.4826 - val_acc: 0.8798\n",
      "Epoch 238/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.2129 - acc: 0.9249 - val_loss: 0.5078 - val_acc: 0.8826\n",
      "Epoch 239/500\n",
      "15786/15786 [==============================] - 10s 610us/step - loss: 0.2024 - acc: 0.9253 - val_loss: 0.5412 - val_acc: 0.8815\n",
      "Epoch 240/500\n",
      "15786/15786 [==============================] - 10s 610us/step - loss: 0.2044 - acc: 0.9261 - val_loss: 0.5217 - val_acc: 0.8786\n",
      "Epoch 241/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.1968 - acc: 0.9268 - val_loss: 0.5934 - val_acc: 0.8707\n",
      "Epoch 242/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.1984 - acc: 0.9280 - val_loss: 0.4986 - val_acc: 0.8832\n",
      "Epoch 243/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1943 - acc: 0.9275 - val_loss: 0.5239 - val_acc: 0.8821\n",
      "Epoch 244/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.2063 - acc: 0.9233 - val_loss: 0.5312 - val_acc: 0.8701\n",
      "Epoch 245/500\n",
      "15786/15786 [==============================] - 10s 606us/step - loss: 0.2023 - acc: 0.9272 - val_loss: 0.5419 - val_acc: 0.8792\n",
      "Epoch 246/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.1941 - acc: 0.9300 - val_loss: 0.5194 - val_acc: 0.8809\n",
      "Epoch 247/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.1994 - acc: 0.9278 - val_loss: 0.4910 - val_acc: 0.8872\n",
      "Epoch 248/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1975 - acc: 0.9258 - val_loss: 0.4944 - val_acc: 0.8803\n",
      "Epoch 249/500\n",
      "15786/15786 [==============================] - 10s 610us/step - loss: 0.2145 - acc: 0.9227 - val_loss: 0.4936 - val_acc: 0.8838\n",
      "Epoch 250/500\n",
      "15786/15786 [==============================] - 10s 606us/step - loss: 0.2044 - acc: 0.9254 - val_loss: 0.4985 - val_acc: 0.8707\n",
      "Epoch 251/500\n",
      "15786/15786 [==============================] - 10s 610us/step - loss: 0.1840 - acc: 0.9304 - val_loss: 0.5434 - val_acc: 0.8866\n",
      "Epoch 252/500\n",
      "15786/15786 [==============================] - 10s 606us/step - loss: 0.2095 - acc: 0.9254 - val_loss: 0.5493 - val_acc: 0.8815\n",
      "Epoch 253/500\n",
      "15786/15786 [==============================] - 10s 606us/step - loss: 0.1954 - acc: 0.9286 - val_loss: 0.6040 - val_acc: 0.8718\n",
      "Epoch 254/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.2070 - acc: 0.9238 - val_loss: 0.4573 - val_acc: 0.8849\n",
      "Epoch 255/500\n",
      "15786/15786 [==============================] - 10s 606us/step - loss: 0.2064 - acc: 0.9259 - val_loss: 0.5271 - val_acc: 0.8729\n",
      "Epoch 256/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.1896 - acc: 0.9310 - val_loss: 0.4995 - val_acc: 0.8866\n",
      "Epoch 257/500\n",
      "15786/15786 [==============================] - 10s 606us/step - loss: 0.1973 - acc: 0.9282 - val_loss: 0.5026 - val_acc: 0.8712\n",
      "Epoch 258/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.1917 - acc: 0.9308 - val_loss: 0.4975 - val_acc: 0.8855\n",
      "Epoch 259/500\n",
      "15786/15786 [==============================] - 10s 610us/step - loss: 0.2008 - acc: 0.9282 - val_loss: 0.4982 - val_acc: 0.8889\n",
      "Epoch 260/500\n",
      "15786/15786 [==============================] - 10s 605us/step - loss: 0.1992 - acc: 0.9261 - val_loss: 0.4912 - val_acc: 0.8855\n",
      "Epoch 261/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1896 - acc: 0.9297 - val_loss: 0.5393 - val_acc: 0.8689\n",
      "Epoch 262/500\n",
      "15786/15786 [==============================] - 10s 606us/step - loss: 0.1919 - acc: 0.9321 - val_loss: 0.5230 - val_acc: 0.8798\n",
      "Epoch 263/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.1909 - acc: 0.9316 - val_loss: 0.4752 - val_acc: 0.8792\n",
      "Epoch 264/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.2028 - acc: 0.9285 - val_loss: 0.5627 - val_acc: 0.8821\n",
      "Epoch 265/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1888 - acc: 0.9296 - val_loss: 0.5298 - val_acc: 0.8729\n",
      "Epoch 266/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1829 - acc: 0.9330 - val_loss: 0.5267 - val_acc: 0.8889\n",
      "Epoch 267/500\n",
      "15786/15786 [==============================] - 10s 606us/step - loss: 0.2020 - acc: 0.9266 - val_loss: 0.4653 - val_acc: 0.8883\n",
      "Epoch 268/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1869 - acc: 0.9304 - val_loss: 0.5402 - val_acc: 0.8718\n",
      "Epoch 269/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1934 - acc: 0.9303 - val_loss: 0.5122 - val_acc: 0.8775\n",
      "Epoch 270/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.1988 - acc: 0.9291 - val_loss: 0.5396 - val_acc: 0.8672\n",
      "Epoch 271/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1900 - acc: 0.9299 - val_loss: 0.5020 - val_acc: 0.8838\n",
      "Epoch 272/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.1988 - acc: 0.9276 - val_loss: 0.4987 - val_acc: 0.8832\n",
      "Epoch 273/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.1947 - acc: 0.9290 - val_loss: 0.4897 - val_acc: 0.8832\n",
      "Epoch 274/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.1930 - acc: 0.9332 - val_loss: 0.4857 - val_acc: 0.8832\n",
      "Epoch 275/500\n",
      "15786/15786 [==============================] - 10s 605us/step - loss: 0.1924 - acc: 0.9302 - val_loss: 0.5332 - val_acc: 0.8832\n",
      "Epoch 276/500\n",
      "15786/15786 [==============================] - 10s 611us/step - loss: 0.1837 - acc: 0.9320 - val_loss: 0.4766 - val_acc: 0.8877\n",
      "Epoch 277/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.2015 - acc: 0.9293 - val_loss: 0.5090 - val_acc: 0.8843\n",
      "Epoch 278/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.2093 - acc: 0.9257 - val_loss: 0.4922 - val_acc: 0.8832\n",
      "Epoch 279/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.1928 - acc: 0.9297 - val_loss: 0.4656 - val_acc: 0.8889\n",
      "Epoch 280/500\n",
      "15786/15786 [==============================] - 10s 606us/step - loss: 0.2072 - acc: 0.9246 - val_loss: 0.4859 - val_acc: 0.8838\n",
      "Epoch 281/500\n",
      "15786/15786 [==============================] - 10s 610us/step - loss: 0.1914 - acc: 0.9292 - val_loss: 0.4671 - val_acc: 0.8872\n",
      "Epoch 282/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.1850 - acc: 0.9342 - val_loss: 0.4715 - val_acc: 0.8843\n",
      "Epoch 283/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1911 - acc: 0.9308 - val_loss: 0.4962 - val_acc: 0.8769\n",
      "Epoch 284/500\n",
      "15786/15786 [==============================] - 10s 610us/step - loss: 0.1925 - acc: 0.9304 - val_loss: 0.4764 - val_acc: 0.8872\n",
      "Epoch 285/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.1822 - acc: 0.9342 - val_loss: 0.4973 - val_acc: 0.8838\n",
      "Epoch 286/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.1908 - acc: 0.9332 - val_loss: 0.5614 - val_acc: 0.8775\n",
      "Epoch 287/500\n",
      "15786/15786 [==============================] - 10s 605us/step - loss: 0.1885 - acc: 0.9309 - val_loss: 0.5226 - val_acc: 0.8821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 288/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.1843 - acc: 0.9336 - val_loss: 0.5010 - val_acc: 0.8775\n",
      "Epoch 289/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.1943 - acc: 0.9298 - val_loss: 0.4935 - val_acc: 0.8849\n",
      "Epoch 290/500\n",
      "15786/15786 [==============================] - 10s 606us/step - loss: 0.2057 - acc: 0.9278 - val_loss: 0.5129 - val_acc: 0.8997\n",
      "Epoch 291/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.1938 - acc: 0.9284 - val_loss: 0.4696 - val_acc: 0.8815\n",
      "Epoch 292/500\n",
      "15786/15786 [==============================] - 10s 605us/step - loss: 0.1917 - acc: 0.9323 - val_loss: 0.4831 - val_acc: 0.8889\n",
      "Epoch 293/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.1930 - acc: 0.9320 - val_loss: 0.4795 - val_acc: 0.8917\n",
      "Epoch 294/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.1881 - acc: 0.9346 - val_loss: 0.4867 - val_acc: 0.8872\n",
      "Epoch 295/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.1883 - acc: 0.9317 - val_loss: 0.4987 - val_acc: 0.8838\n",
      "Epoch 296/500\n",
      "15786/15786 [==============================] - 10s 610us/step - loss: 0.1842 - acc: 0.9315 - val_loss: 0.5182 - val_acc: 0.8969\n",
      "Epoch 297/500\n",
      "15786/15786 [==============================] - 10s 605us/step - loss: 0.1891 - acc: 0.9302 - val_loss: 0.4920 - val_acc: 0.8923\n",
      "Epoch 298/500\n",
      "15786/15786 [==============================] - 10s 627us/step - loss: 0.1915 - acc: 0.9318 - val_loss: 0.5323 - val_acc: 0.8809\n",
      "Epoch 299/500\n",
      "15786/15786 [==============================] - 10s 610us/step - loss: 0.1990 - acc: 0.9299 - val_loss: 0.4748 - val_acc: 0.8889\n",
      "Epoch 300/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.2000 - acc: 0.9294 - val_loss: 0.5007 - val_acc: 0.8883\n",
      "Epoch 301/500\n",
      "15786/15786 [==============================] - 10s 610us/step - loss: 0.1979 - acc: 0.9313 - val_loss: 0.6091 - val_acc: 0.8678\n",
      "Epoch 302/500\n",
      "15786/15786 [==============================] - 10s 606us/step - loss: 0.1905 - acc: 0.9323 - val_loss: 0.5009 - val_acc: 0.8855\n",
      "Epoch 303/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1967 - acc: 0.9323 - val_loss: 0.5575 - val_acc: 0.8798\n",
      "Epoch 304/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1954 - acc: 0.9286 - val_loss: 0.4853 - val_acc: 0.8866\n",
      "Epoch 305/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.1833 - acc: 0.9351 - val_loss: 0.5025 - val_acc: 0.8855\n",
      "Epoch 306/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1968 - acc: 0.9316 - val_loss: 0.4866 - val_acc: 0.8752\n",
      "Epoch 307/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.1920 - acc: 0.9308 - val_loss: 0.5361 - val_acc: 0.8849\n",
      "Epoch 308/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1785 - acc: 0.9341 - val_loss: 0.5104 - val_acc: 0.8803\n",
      "Epoch 309/500\n",
      "15786/15786 [==============================] - 10s 610us/step - loss: 0.1843 - acc: 0.9358 - val_loss: 0.4868 - val_acc: 0.8895\n",
      "Epoch 310/500\n",
      "15786/15786 [==============================] - 10s 605us/step - loss: 0.1881 - acc: 0.9311 - val_loss: 0.5823 - val_acc: 0.8866\n",
      "Epoch 311/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1890 - acc: 0.9335 - val_loss: 0.4620 - val_acc: 0.8929\n",
      "Epoch 312/500\n",
      "15786/15786 [==============================] - 10s 606us/step - loss: 0.1766 - acc: 0.9345 - val_loss: 0.4841 - val_acc: 0.8952\n",
      "Epoch 313/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.1738 - acc: 0.9358 - val_loss: 0.5281 - val_acc: 0.8883\n",
      "Epoch 314/500\n",
      "15786/15786 [==============================] - 10s 611us/step - loss: 0.1845 - acc: 0.9368 - val_loss: 0.5050 - val_acc: 0.8843\n",
      "Epoch 315/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.1908 - acc: 0.9318 - val_loss: 0.5313 - val_acc: 0.8883\n",
      "Epoch 316/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1850 - acc: 0.9321 - val_loss: 0.4887 - val_acc: 0.8860\n",
      "Epoch 317/500\n",
      "15786/15786 [==============================] - 10s 606us/step - loss: 0.1753 - acc: 0.9345 - val_loss: 0.4907 - val_acc: 0.8838\n",
      "Epoch 318/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1682 - acc: 0.9369 - val_loss: 0.5109 - val_acc: 0.8895\n",
      "Epoch 319/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1999 - acc: 0.9284 - val_loss: 0.5041 - val_acc: 0.8758\n",
      "Epoch 320/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.1819 - acc: 0.9319 - val_loss: 0.5661 - val_acc: 0.8872\n",
      "Epoch 321/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1864 - acc: 0.9321 - val_loss: 0.5571 - val_acc: 0.8843\n",
      "Epoch 322/500\n",
      "15786/15786 [==============================] - 10s 606us/step - loss: 0.1772 - acc: 0.9350 - val_loss: 0.4860 - val_acc: 0.8980\n",
      "Epoch 323/500\n",
      "15786/15786 [==============================] - 10s 610us/step - loss: 0.1969 - acc: 0.9301 - val_loss: 0.5586 - val_acc: 0.8832\n",
      "Epoch 324/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1731 - acc: 0.9374 - val_loss: 0.5032 - val_acc: 0.8809\n",
      "Epoch 325/500\n",
      "15786/15786 [==============================] - 10s 606us/step - loss: 0.1750 - acc: 0.9370 - val_loss: 0.4954 - val_acc: 0.8940\n",
      "Epoch 326/500\n",
      "15786/15786 [==============================] - 10s 610us/step - loss: 0.1899 - acc: 0.9324 - val_loss: 0.4775 - val_acc: 0.8843\n",
      "Epoch 327/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.1800 - acc: 0.9354 - val_loss: 0.4993 - val_acc: 0.8860\n",
      "Epoch 328/500\n",
      "15786/15786 [==============================] - 10s 610us/step - loss: 0.1825 - acc: 0.9356 - val_loss: 0.5384 - val_acc: 0.8849\n",
      "Epoch 329/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.1991 - acc: 0.9297 - val_loss: 0.4920 - val_acc: 0.8923\n",
      "Epoch 330/500\n",
      "15786/15786 [==============================] - 10s 606us/step - loss: 0.1954 - acc: 0.9327 - val_loss: 0.4687 - val_acc: 0.8860\n",
      "Epoch 331/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.1820 - acc: 0.9340 - val_loss: 0.5191 - val_acc: 0.8855\n",
      "Epoch 332/500\n",
      "15786/15786 [==============================] - 10s 606us/step - loss: 0.1961 - acc: 0.9316 - val_loss: 0.5215 - val_acc: 0.8781\n",
      "Epoch 333/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.1809 - acc: 0.9367 - val_loss: 0.5230 - val_acc: 0.8849\n",
      "Epoch 334/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.1900 - acc: 0.9318 - val_loss: 0.4729 - val_acc: 0.8866\n",
      "Epoch 335/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.1873 - acc: 0.9313 - val_loss: 0.5247 - val_acc: 0.8866\n",
      "Epoch 336/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.1837 - acc: 0.9333 - val_loss: 0.5207 - val_acc: 0.8883\n",
      "Epoch 337/500\n",
      "15786/15786 [==============================] - 10s 606us/step - loss: 0.1881 - acc: 0.9320 - val_loss: 0.5280 - val_acc: 0.8889\n",
      "Epoch 338/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1821 - acc: 0.9334 - val_loss: 0.4747 - val_acc: 0.8912\n",
      "Epoch 339/500\n",
      "15786/15786 [==============================] - 10s 610us/step - loss: 0.1740 - acc: 0.9365 - val_loss: 0.4489 - val_acc: 0.8946\n",
      "Epoch 340/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.1808 - acc: 0.9349 - val_loss: 0.5391 - val_acc: 0.8889\n",
      "Epoch 341/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1822 - acc: 0.9352 - val_loss: 0.5108 - val_acc: 0.8940\n",
      "Epoch 342/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.1756 - acc: 0.9377 - val_loss: 0.5349 - val_acc: 0.8917\n",
      "Epoch 343/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1908 - acc: 0.9315 - val_loss: 0.5432 - val_acc: 0.8906\n",
      "Epoch 344/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.1683 - acc: 0.9399 - val_loss: 0.5387 - val_acc: 0.8838\n",
      "Epoch 345/500\n",
      "15786/15786 [==============================] - 10s 605us/step - loss: 0.1799 - acc: 0.9363 - val_loss: 0.5627 - val_acc: 0.8832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 346/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.1946 - acc: 0.9294 - val_loss: 0.5134 - val_acc: 0.8986\n",
      "Epoch 347/500\n",
      "15786/15786 [==============================] - 10s 605us/step - loss: 0.1800 - acc: 0.9320 - val_loss: 0.4747 - val_acc: 0.8974\n",
      "Epoch 348/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1787 - acc: 0.9387 - val_loss: 0.4958 - val_acc: 0.8986\n",
      "Epoch 349/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.1746 - acc: 0.9397 - val_loss: 0.5303 - val_acc: 0.8940\n",
      "Epoch 350/500\n",
      "15786/15786 [==============================] - 10s 605us/step - loss: 0.1930 - acc: 0.9330 - val_loss: 0.5111 - val_acc: 0.8917\n",
      "Epoch 351/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1929 - acc: 0.9311 - val_loss: 0.4885 - val_acc: 0.8963\n",
      "Epoch 352/500\n",
      "15786/15786 [==============================] - 10s 605us/step - loss: 0.1735 - acc: 0.9404 - val_loss: 0.5608 - val_acc: 0.8934\n",
      "Epoch 353/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1980 - acc: 0.9304 - val_loss: 0.5598 - val_acc: 0.8883\n",
      "Epoch 354/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.1839 - acc: 0.9355 - val_loss: 0.5423 - val_acc: 0.8832\n",
      "Epoch 355/500\n",
      "15786/15786 [==============================] - 10s 606us/step - loss: 0.1790 - acc: 0.9337 - val_loss: 0.4914 - val_acc: 0.8889\n",
      "Epoch 356/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.1863 - acc: 0.9325 - val_loss: 0.4810 - val_acc: 0.8877\n",
      "Epoch 357/500\n",
      "15786/15786 [==============================] - 10s 606us/step - loss: 0.1799 - acc: 0.9361 - val_loss: 0.4859 - val_acc: 0.8900\n",
      "Epoch 358/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.1807 - acc: 0.9361 - val_loss: 0.5175 - val_acc: 0.8929\n",
      "Epoch 359/500\n",
      "15786/15786 [==============================] - 10s 614us/step - loss: 0.1730 - acc: 0.9355 - val_loss: 0.5354 - val_acc: 0.8963\n",
      "Epoch 360/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.1837 - acc: 0.9346 - val_loss: 0.5354 - val_acc: 0.8860\n",
      "Epoch 361/500\n",
      "15786/15786 [==============================] - 10s 610us/step - loss: 0.1833 - acc: 0.9340 - val_loss: 0.5051 - val_acc: 0.8889\n",
      "Epoch 362/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.1815 - acc: 0.9358 - val_loss: 0.5115 - val_acc: 0.8877\n",
      "Epoch 363/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1738 - acc: 0.9370 - val_loss: 0.5415 - val_acc: 0.8769\n",
      "Epoch 364/500\n",
      "15786/15786 [==============================] - 10s 611us/step - loss: 0.1790 - acc: 0.9335 - val_loss: 0.4920 - val_acc: 0.8923\n",
      "Epoch 365/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.1821 - acc: 0.9358 - val_loss: 0.5139 - val_acc: 0.8735\n",
      "Epoch 366/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.1769 - acc: 0.9351 - val_loss: 0.4737 - val_acc: 0.8906\n",
      "Epoch 367/500\n",
      "15786/15786 [==============================] - 10s 606us/step - loss: 0.1697 - acc: 0.9420 - val_loss: 0.4751 - val_acc: 0.9043\n",
      "Epoch 368/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1738 - acc: 0.9386 - val_loss: 0.4880 - val_acc: 0.8815\n",
      "Epoch 369/500\n",
      "15786/15786 [==============================] - 10s 627us/step - loss: 0.1662 - acc: 0.9396 - val_loss: 0.5062 - val_acc: 0.8838\n",
      "Epoch 370/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.1791 - acc: 0.9367 - val_loss: 0.5429 - val_acc: 0.8929\n",
      "Epoch 371/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.1913 - acc: 0.9341 - val_loss: 0.4756 - val_acc: 0.8986\n",
      "Epoch 372/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.1819 - acc: 0.9351 - val_loss: 0.5236 - val_acc: 0.8883\n",
      "Epoch 373/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.1753 - acc: 0.9361 - val_loss: 0.4722 - val_acc: 0.9003\n",
      "Epoch 374/500\n",
      "15786/15786 [==============================] - 10s 610us/step - loss: 0.1697 - acc: 0.9378 - val_loss: 0.5432 - val_acc: 0.8877\n",
      "Epoch 375/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.1639 - acc: 0.9400 - val_loss: 0.5007 - val_acc: 0.8900\n",
      "Epoch 376/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1732 - acc: 0.9377 - val_loss: 0.4882 - val_acc: 0.8889\n",
      "Epoch 377/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.1786 - acc: 0.9383 - val_loss: 0.5251 - val_acc: 0.8917\n",
      "Epoch 378/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1752 - acc: 0.9377 - val_loss: 0.4933 - val_acc: 0.8781\n",
      "Epoch 379/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1663 - acc: 0.9361 - val_loss: 0.5594 - val_acc: 0.8855\n",
      "Epoch 380/500\n",
      "15786/15786 [==============================] - 10s 605us/step - loss: 0.1798 - acc: 0.9391 - val_loss: 0.5121 - val_acc: 0.8860\n",
      "Epoch 381/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1944 - acc: 0.9339 - val_loss: 0.5282 - val_acc: 0.8877\n",
      "Epoch 382/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.1864 - acc: 0.9326 - val_loss: 0.4842 - val_acc: 0.8889\n",
      "Epoch 383/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1830 - acc: 0.9355 - val_loss: 0.4936 - val_acc: 0.8838\n",
      "Epoch 384/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.1862 - acc: 0.9337 - val_loss: 0.5213 - val_acc: 0.8843\n",
      "Epoch 385/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.1779 - acc: 0.9362 - val_loss: 0.5963 - val_acc: 0.8803\n",
      "Epoch 386/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.1817 - acc: 0.9360 - val_loss: 0.5621 - val_acc: 0.8769\n",
      "Epoch 387/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1932 - acc: 0.9334 - val_loss: 0.5201 - val_acc: 0.8689\n",
      "Epoch 388/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1821 - acc: 0.9375 - val_loss: 0.4791 - val_acc: 0.8877\n",
      "Epoch 389/500\n",
      "15786/15786 [==============================] - 10s 610us/step - loss: 0.1736 - acc: 0.9344 - val_loss: 0.5027 - val_acc: 0.8883\n",
      "Epoch 390/500\n",
      "15786/15786 [==============================] - 10s 615us/step - loss: 0.1752 - acc: 0.9380 - val_loss: 0.4804 - val_acc: 0.8929\n",
      "Epoch 391/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.1748 - acc: 0.9371 - val_loss: 0.4813 - val_acc: 0.8912\n",
      "Epoch 392/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.1692 - acc: 0.9391 - val_loss: 0.4900 - val_acc: 0.8912\n",
      "Epoch 393/500\n",
      "15786/15786 [==============================] - 10s 610us/step - loss: 0.1820 - acc: 0.9351 - val_loss: 0.5075 - val_acc: 0.8889\n",
      "Epoch 394/500\n",
      "15786/15786 [==============================] - 10s 610us/step - loss: 0.1726 - acc: 0.9383 - val_loss: 0.5369 - val_acc: 0.8906\n",
      "Epoch 395/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.1918 - acc: 0.9343 - val_loss: 0.5264 - val_acc: 0.8855\n",
      "Epoch 396/500\n",
      "15786/15786 [==============================] - 10s 610us/step - loss: 0.1747 - acc: 0.9357 - val_loss: 0.5987 - val_acc: 0.8809\n",
      "Epoch 397/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.1972 - acc: 0.9323 - val_loss: 0.5580 - val_acc: 0.8849\n",
      "Epoch 398/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.1961 - acc: 0.9292 - val_loss: 0.5096 - val_acc: 0.8843\n",
      "Epoch 399/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.1746 - acc: 0.9373 - val_loss: 0.4980 - val_acc: 0.8906\n",
      "Epoch 400/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.1764 - acc: 0.9376 - val_loss: 0.5033 - val_acc: 0.8866\n",
      "Epoch 401/500\n",
      "15786/15786 [==============================] - 10s 611us/step - loss: 0.1732 - acc: 0.9376 - val_loss: 0.5385 - val_acc: 0.8821\n",
      "Epoch 402/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.1685 - acc: 0.9378 - val_loss: 0.5189 - val_acc: 0.8934\n",
      "Epoch 403/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1792 - acc: 0.9343 - val_loss: 0.5818 - val_acc: 0.8860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 404/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.1768 - acc: 0.9371 - val_loss: 0.5049 - val_acc: 0.8940\n",
      "Epoch 405/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.1811 - acc: 0.9347 - val_loss: 0.4921 - val_acc: 0.8957\n",
      "Epoch 406/500\n",
      "15786/15786 [==============================] - 10s 610us/step - loss: 0.1758 - acc: 0.9364 - val_loss: 0.5950 - val_acc: 0.8786\n",
      "Epoch 407/500\n",
      "15786/15786 [==============================] - 10s 605us/step - loss: 0.1799 - acc: 0.9368 - val_loss: 0.5016 - val_acc: 0.8860\n",
      "Epoch 408/500\n",
      "15786/15786 [==============================] - 10s 611us/step - loss: 0.1644 - acc: 0.9409 - val_loss: 0.4870 - val_acc: 0.8963\n",
      "Epoch 409/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1759 - acc: 0.9388 - val_loss: 0.5640 - val_acc: 0.8877\n",
      "Epoch 410/500\n",
      "15786/15786 [==============================] - 10s 605us/step - loss: 0.1838 - acc: 0.9341 - val_loss: 0.5083 - val_acc: 0.8849\n",
      "Epoch 411/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.1709 - acc: 0.9402 - val_loss: 0.4966 - val_acc: 0.8934\n",
      "Epoch 412/500\n",
      "15786/15786 [==============================] - 10s 606us/step - loss: 0.1781 - acc: 0.9368 - val_loss: 0.4925 - val_acc: 0.8838\n",
      "Epoch 413/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1685 - acc: 0.9382 - val_loss: 0.4797 - val_acc: 0.8974\n",
      "Epoch 414/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1761 - acc: 0.9382 - val_loss: 0.5500 - val_acc: 0.8826\n",
      "Epoch 415/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.1596 - acc: 0.9443 - val_loss: 0.6307 - val_acc: 0.8621\n",
      "Epoch 416/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.1665 - acc: 0.9389 - val_loss: 0.5006 - val_acc: 0.8923\n",
      "Epoch 417/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.1759 - acc: 0.9391 - val_loss: 0.6697 - val_acc: 0.8775\n",
      "Epoch 418/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1740 - acc: 0.9387 - val_loss: 0.5106 - val_acc: 0.8917\n",
      "Epoch 419/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1687 - acc: 0.9399 - val_loss: 0.5309 - val_acc: 0.8895\n",
      "Epoch 420/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.1802 - acc: 0.9346 - val_loss: 0.5205 - val_acc: 0.8764\n",
      "Epoch 421/500\n",
      "15786/15786 [==============================] - 10s 612us/step - loss: 0.1916 - acc: 0.9339 - val_loss: 0.5483 - val_acc: 0.8906\n",
      "Epoch 422/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.1679 - acc: 0.9399 - val_loss: 0.5838 - val_acc: 0.8803\n",
      "Epoch 423/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.2008 - acc: 0.9318 - val_loss: 0.4857 - val_acc: 0.8912\n",
      "Epoch 424/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1744 - acc: 0.9368 - val_loss: 0.5390 - val_acc: 0.8912\n",
      "Epoch 425/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.1863 - acc: 0.9349 - val_loss: 0.5734 - val_acc: 0.8821\n",
      "Epoch 426/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1803 - acc: 0.9380 - val_loss: 0.4953 - val_acc: 0.8969\n",
      "Epoch 427/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.1675 - acc: 0.9399 - val_loss: 0.5347 - val_acc: 0.8917\n",
      "Epoch 428/500\n",
      "15786/15786 [==============================] - 10s 610us/step - loss: 0.1680 - acc: 0.9405 - val_loss: 0.5424 - val_acc: 0.8895\n",
      "Epoch 429/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1804 - acc: 0.9381 - val_loss: 0.5648 - val_acc: 0.8934\n",
      "Epoch 430/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.1644 - acc: 0.9415 - val_loss: 0.5306 - val_acc: 0.8974\n",
      "Epoch 431/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1669 - acc: 0.9422 - val_loss: 0.5392 - val_acc: 0.8895\n",
      "Epoch 432/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.1645 - acc: 0.9410 - val_loss: 0.4973 - val_acc: 0.8832\n",
      "Epoch 433/500\n",
      "15786/15786 [==============================] - 10s 610us/step - loss: 0.1785 - acc: 0.9381 - val_loss: 0.5250 - val_acc: 0.8877\n",
      "Epoch 434/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.1825 - acc: 0.9356 - val_loss: 0.4657 - val_acc: 0.9054\n",
      "Epoch 435/500\n",
      "15786/15786 [==============================] - 10s 606us/step - loss: 0.1821 - acc: 0.9352 - val_loss: 0.4387 - val_acc: 0.9031\n",
      "Epoch 436/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1591 - acc: 0.9427 - val_loss: 0.5470 - val_acc: 0.8792\n",
      "Epoch 437/500\n",
      "15786/15786 [==============================] - 10s 605us/step - loss: 0.1602 - acc: 0.9413 - val_loss: 0.5498 - val_acc: 0.8917\n",
      "Epoch 438/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1838 - acc: 0.9360 - val_loss: 0.4812 - val_acc: 0.8991\n",
      "Epoch 439/500\n",
      "15786/15786 [==============================] - 10s 610us/step - loss: 0.1690 - acc: 0.9389 - val_loss: 0.5192 - val_acc: 0.8821\n",
      "Epoch 440/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.1812 - acc: 0.9367 - val_loss: 0.5440 - val_acc: 0.8752\n",
      "Epoch 441/500\n",
      "15786/15786 [==============================] - 10s 610us/step - loss: 0.1881 - acc: 0.9325 - val_loss: 0.5378 - val_acc: 0.8866\n",
      "Epoch 442/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.1912 - acc: 0.9332 - val_loss: 0.5305 - val_acc: 0.8889\n",
      "Epoch 443/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1808 - acc: 0.9359 - val_loss: 0.5180 - val_acc: 0.8929\n",
      "Epoch 444/500\n",
      "15786/15786 [==============================] - 10s 610us/step - loss: 0.1734 - acc: 0.9384 - val_loss: 0.5272 - val_acc: 0.8889\n",
      "Epoch 445/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.1606 - acc: 0.9431 - val_loss: 0.5318 - val_acc: 0.8986\n",
      "Epoch 446/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.1750 - acc: 0.9401 - val_loss: 0.5307 - val_acc: 0.8866\n",
      "Epoch 447/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.1766 - acc: 0.9364 - val_loss: 0.5248 - val_acc: 0.8963\n",
      "Epoch 448/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.1634 - acc: 0.9450 - val_loss: 0.5069 - val_acc: 0.8997\n",
      "Epoch 449/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.1795 - acc: 0.9373 - val_loss: 0.5070 - val_acc: 0.8895\n",
      "Epoch 450/500\n",
      "15786/15786 [==============================] - 10s 606us/step - loss: 0.1712 - acc: 0.9405 - val_loss: 0.5328 - val_acc: 0.8849\n",
      "Epoch 451/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1661 - acc: 0.9394 - val_loss: 0.6146 - val_acc: 0.8895\n",
      "Epoch 452/500\n",
      "15786/15786 [==============================] - 10s 614us/step - loss: 0.1613 - acc: 0.9427 - val_loss: 0.5752 - val_acc: 0.8963\n",
      "Epoch 453/500\n",
      "15786/15786 [==============================] - 10s 619us/step - loss: 0.1888 - acc: 0.9364 - val_loss: 0.4775 - val_acc: 0.8912\n",
      "Epoch 454/500\n",
      "15786/15786 [==============================] - 10s 610us/step - loss: 0.1617 - acc: 0.9419 - val_loss: 0.5674 - val_acc: 0.8906\n",
      "Epoch 455/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.1765 - acc: 0.9375 - val_loss: 0.5163 - val_acc: 0.8895\n",
      "Epoch 456/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1686 - acc: 0.9398 - val_loss: 0.4867 - val_acc: 0.8952\n",
      "Epoch 457/500\n",
      "15786/15786 [==============================] - 10s 606us/step - loss: 0.1628 - acc: 0.9428 - val_loss: 0.4832 - val_acc: 0.8929\n",
      "Epoch 458/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.1668 - acc: 0.9410 - val_loss: 0.5382 - val_acc: 0.8912\n",
      "Epoch 459/500\n",
      "15786/15786 [==============================] - 10s 610us/step - loss: 0.1609 - acc: 0.9424 - val_loss: 0.5372 - val_acc: 0.8917\n",
      "Epoch 460/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.1746 - acc: 0.9368 - val_loss: 0.5174 - val_acc: 0.8952\n",
      "Epoch 461/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.1674 - acc: 0.9400 - val_loss: 0.5514 - val_acc: 0.8798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 462/500\n",
      "15786/15786 [==============================] - 10s 606us/step - loss: 0.1764 - acc: 0.9399 - val_loss: 0.6388 - val_acc: 0.8769\n",
      "Epoch 463/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.1761 - acc: 0.9386 - val_loss: 0.4823 - val_acc: 0.8991\n",
      "Epoch 464/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1745 - acc: 0.9384 - val_loss: 0.5202 - val_acc: 0.8917\n",
      "Epoch 465/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.1793 - acc: 0.9377 - val_loss: 0.5083 - val_acc: 0.8963\n",
      "Epoch 466/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.1736 - acc: 0.9373 - val_loss: 0.5158 - val_acc: 0.8689\n",
      "Epoch 467/500\n",
      "15786/15786 [==============================] - 10s 606us/step - loss: 0.1844 - acc: 0.9357 - val_loss: 0.5280 - val_acc: 0.8900\n",
      "Epoch 468/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1666 - acc: 0.9415 - val_loss: 0.5115 - val_acc: 0.8843\n",
      "Epoch 469/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.1594 - acc: 0.9449 - val_loss: 0.5037 - val_acc: 0.8917\n",
      "Epoch 470/500\n",
      "15786/15786 [==============================] - 10s 606us/step - loss: 0.1715 - acc: 0.9414 - val_loss: 0.5692 - val_acc: 0.8838\n",
      "Epoch 471/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.1621 - acc: 0.9426 - val_loss: 0.4862 - val_acc: 0.8952\n",
      "Epoch 472/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.1691 - acc: 0.9405 - val_loss: 0.5365 - val_acc: 0.8969\n",
      "Epoch 473/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.1781 - acc: 0.9396 - val_loss: 0.5046 - val_acc: 0.8986\n",
      "Epoch 474/500\n",
      "15786/15786 [==============================] - 10s 611us/step - loss: 0.1755 - acc: 0.9401 - val_loss: 0.5336 - val_acc: 0.8781\n",
      "Epoch 475/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.1543 - acc: 0.9456 - val_loss: 0.5214 - val_acc: 0.8889\n",
      "Epoch 476/500\n",
      "15786/15786 [==============================] - 10s 610us/step - loss: 0.1703 - acc: 0.9394 - val_loss: 0.5377 - val_acc: 0.8934\n",
      "Epoch 477/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.1749 - acc: 0.9402 - val_loss: 0.5369 - val_acc: 0.8877\n",
      "Epoch 478/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1675 - acc: 0.9410 - val_loss: 0.4742 - val_acc: 0.8866\n",
      "Epoch 479/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1709 - acc: 0.9410 - val_loss: 0.4928 - val_acc: 0.8963\n",
      "Epoch 480/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.1706 - acc: 0.9419 - val_loss: 0.5145 - val_acc: 0.8917\n",
      "Epoch 481/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1675 - acc: 0.9391 - val_loss: 0.5084 - val_acc: 0.8877\n",
      "Epoch 482/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.1585 - acc: 0.9423 - val_loss: 0.5084 - val_acc: 0.8912\n",
      "Epoch 483/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.1731 - acc: 0.9405 - val_loss: 0.5099 - val_acc: 0.8963\n",
      "Epoch 484/500\n",
      "15786/15786 [==============================] - 10s 612us/step - loss: 0.1729 - acc: 0.9403 - val_loss: 0.5497 - val_acc: 0.8872\n",
      "Epoch 485/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.1581 - acc: 0.9437 - val_loss: 0.4885 - val_acc: 0.8952\n",
      "Epoch 486/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1668 - acc: 0.9412 - val_loss: 0.5000 - val_acc: 0.8934\n",
      "Epoch 487/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.1615 - acc: 0.9422 - val_loss: 0.5058 - val_acc: 0.8991\n",
      "Epoch 488/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.1686 - acc: 0.9406 - val_loss: 0.5286 - val_acc: 0.8952\n",
      "Epoch 489/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1637 - acc: 0.9404 - val_loss: 0.4948 - val_acc: 0.8895\n",
      "Epoch 490/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.1576 - acc: 0.9423 - val_loss: 0.5293 - val_acc: 0.8855\n",
      "Epoch 491/500\n",
      "15786/15786 [==============================] - 10s 611us/step - loss: 0.1740 - acc: 0.9384 - val_loss: 0.4950 - val_acc: 0.8832\n",
      "Epoch 492/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.1683 - acc: 0.9424 - val_loss: 0.4930 - val_acc: 0.8860\n",
      "Epoch 493/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1714 - acc: 0.9391 - val_loss: 0.5178 - val_acc: 0.8900\n",
      "Epoch 494/500\n",
      "15786/15786 [==============================] - 10s 610us/step - loss: 0.1633 - acc: 0.9427 - val_loss: 0.5170 - val_acc: 0.8746\n",
      "Epoch 495/500\n",
      "15786/15786 [==============================] - 10s 606us/step - loss: 0.1615 - acc: 0.9431 - val_loss: 0.5731 - val_acc: 0.8701\n",
      "Epoch 496/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1735 - acc: 0.9411 - val_loss: 0.5095 - val_acc: 0.8986\n",
      "Epoch 497/500\n",
      "15786/15786 [==============================] - 10s 607us/step - loss: 0.1788 - acc: 0.9372 - val_loss: 0.5312 - val_acc: 0.8963\n",
      "Epoch 498/500\n",
      "15786/15786 [==============================] - 10s 608us/step - loss: 0.1689 - acc: 0.9415 - val_loss: 0.5159 - val_acc: 0.8900\n",
      "Epoch 499/500\n",
      "15786/15786 [==============================] - 10s 609us/step - loss: 0.1694 - acc: 0.9415 - val_loss: 0.4866 - val_acc: 0.8946\n",
      "Epoch 500/500\n",
      "15786/15786 [==============================] - 10s 606us/step - loss: 0.1656 - acc: 0.9396 - val_loss: 0.5169 - val_acc: 0.8889\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 500\n",
    "history = model.fit(X_train2, y_train2,\n",
    "         batch_size=batch_size,\n",
    "         epochs=epochs,\n",
    "         verbose=1,\n",
    "         validation_data=(X_val, y_val),\n",
    "         shuffle=True,\n",
    "         callbacks=[\n",
    "             #LearningRateScheduler(lr_schedule),\n",
    "             ModelCheckpoint('classifier_lab1102.h5', save_best_only=True)\n",
    "         ]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_train_history(history, train_metrics, val_metrics):\n",
    "    plt.plot(history.history.get(train_metrics),'-o')\n",
    "    plt.plot(history.history.get(val_metrics),'-o')\n",
    "    plt.ylabel(train_metrics)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend(['train', 'validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "plot_train_history(history, 'loss','val_loss')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plot_train_history(history, 'acc','val_acc')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1950/1950 [==============================] - 0s 188us/step\n"
     ]
    }
   ],
   "source": [
    "#X_test = np.expand_dims(X_test, axis=-1)\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "loss,accuracy_test=model.evaluate(X_test,y_test)\n",
    "y_pred=model.predict(X_test) \n",
    "y_pred=np.argmax(y_pred, axis=1)\n",
    "y_test=np.argmax(y_test, axis=1)\n",
    "cm_test = confusion_matrix(y_test, y_pred)\n",
    "#recall_test = recall_score(y_test,y_pred,average='micro')\n",
    "#precision_test = precision_score(y_test,y_pred,average='micro')\n",
    "#f1_score_test = f1_score(y_test,y_pred,average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[424,   4,  24,  36],\n",
       "       [  5, 460,  28,  15],\n",
       "       [ 32,  16, 408,  21],\n",
       "       [ 16,  20,  10, 431]], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8888888888888888 0.92 0.8680851063829788 0.856858846918489\n"
     ]
    }
   ],
   "source": [
    "#recall test\n",
    "recall_R_test=cm_test[0][0]/(cm_test[0][0]+cm_test[1][0]+cm_test[2][0]+cm_test[3][0])\n",
    "recall_L_test=cm_test[1][1]/(cm_test[0][1]+cm_test[1][1]+cm_test[2][1]+cm_test[3][1])\n",
    "recall_F_test=cm_test[2][2]/(cm_test[0][2]+cm_test[1][2]+cm_test[2][2]+cm_test[3][2])\n",
    "recall_B_test=cm_test[3][3]/(cm_test[0][3]+cm_test[1][3]+cm_test[2][3]+cm_test[3][3])\n",
    "print(recall_R_test,recall_L_test,recall_F_test,recall_B_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8688524590163934 0.905511811023622 0.8553459119496856 0.9035639412997903\n"
     ]
    }
   ],
   "source": [
    "#precision test\n",
    "precision_R_test=cm_test[0][0]/(cm_test[0][0]+cm_test[0][1]+cm_test[0][2]+cm_test[0][3])\n",
    "precision_L_test=cm_test[1][1]/(cm_test[1][0]+cm_test[1][1]+cm_test[1][2]+cm_test[1][3])\n",
    "precision_F_test=cm_test[2][2]/(cm_test[2][0]+cm_test[2][1]+cm_test[2][2]+cm_test[2][3])\n",
    "precision_B_test=cm_test[3][3]/(cm_test[3][0]+cm_test[3][1]+cm_test[3][2]+cm_test[3][3])\n",
    "print(precision_R_test,precision_L_test,precision_F_test,precision_B_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8787564766839379 0.9126984126984128 0.8616684266103484 0.8795918367346939\n"
     ]
    }
   ],
   "source": [
    "#f1score test\n",
    "f1score_R_test=2/((1/precision_R_test)+1/recall_R_test)\n",
    "f1score_L_test=2/((1/precision_L_test)+1/recall_L_test)\n",
    "f1score_F_test=2/((1/precision_F_test)+1/recall_F_test)\n",
    "f1score_B_test=2/((1/precision_B_test)+1/recall_B_test)\n",
    "print(f1score_R_test,f1score_L_test,f1score_F_test,f1score_B_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8835897432229458"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#accuary test\n",
    "accuracy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1755/1755 [==============================] - 0s 182us/step\n"
     ]
    }
   ],
   "source": [
    "loss_val,accuracy_val=model.evaluate(X_val,y_val)\n",
    "yyy_pred=model.predict(X_val) \n",
    "yyy_pred=np.argmax(yyy_pred, axis=1)\n",
    "y_val=np.argmax(y_val, axis=1)\n",
    "cm_val = confusion_matrix(y_val, yyy_pred)\n",
    "#recall_val = recall_score(y_val,yyy_pred,average='micro')\n",
    "#precision_val = precision_score(y_val,yyy_pred,average='micro')\n",
    "#f1_score_val = f1_score(y_val,yyy_pred,average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[400,   4,  14,  30],\n",
       "       [ 11, 343,  23,  19],\n",
       "       [ 31,  18, 374,  13],\n",
       "       [  6,  12,  14, 443]], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8928571428571429 0.9098143236074271 0.88 0.8772277227722772\n"
     ]
    }
   ],
   "source": [
    "#recall val\n",
    "recall_R_val=cm_val[0][0]/(cm_val[0][0]+cm_val[1][0]+cm_val[2][0]+cm_val[3][0])\n",
    "recall_L_val=cm_val[1][1]/(cm_val[0][1]+cm_val[1][1]+cm_val[2][1]+cm_val[3][1])\n",
    "recall_F_val=cm_val[2][2]/(cm_val[0][2]+cm_val[1][2]+cm_val[2][2]+cm_val[3][2])\n",
    "recall_B_val=cm_val[3][3]/(cm_val[0][3]+cm_val[1][3]+cm_val[2][3]+cm_val[3][3])\n",
    "print(recall_R_val,recall_L_val,recall_F_val,recall_B_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8928571428571429 0.8661616161616161 0.8577981651376146 0.9326315789473684\n"
     ]
    }
   ],
   "source": [
    "#precision val\n",
    "precision_R_val=cm_val[0][0]/(cm_val[0][0]+cm_val[0][1]+cm_val[0][2]+cm_val[0][3])\n",
    "precision_L_val=cm_val[1][1]/(cm_val[1][0]+cm_val[1][1]+cm_val[1][2]+cm_val[1][3])\n",
    "precision_F_val=cm_val[2][2]/(cm_val[2][0]+cm_val[2][1]+cm_val[2][2]+cm_val[2][3])\n",
    "precision_B_val=cm_val[3][3]/(cm_val[3][0]+cm_val[3][1]+cm_val[3][2]+cm_val[3][3])\n",
    "print(precision_R_val,precision_L_val,precision_F_val,precision_B_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8928571428571429 0.8874514877102199 0.8687572590011614 0.9040816326530613\n"
     ]
    }
   ],
   "source": [
    "#f1score val\n",
    "f1score_R_val=2/((1/precision_R_val)+1/recall_R_val)\n",
    "f1score_L_val=2/((1/precision_L_val)+1/recall_L_val)\n",
    "f1score_F_val=2/((1/precision_F_val)+1/recall_F_val)\n",
    "f1score_B_val=2/((1/precision_B_val)+1/recall_B_val)\n",
    "print(f1score_R_val,f1score_L_val,f1score_F_val,f1score_B_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8888888892285165"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#accuary val\n",
    "accuracy_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17541/17541 [==============================] - 3s 186us/step\n"
     ]
    }
   ],
   "source": [
    "loss_train,accuracy_train=model.evaluate(X_train,y_train)\n",
    "yy_pred=model.predict(X_train) \n",
    "yy_pred=np.argmax(yy_pred, axis=1)\n",
    "y_train=np.argmax(y_train, axis=1)\n",
    "cm_train = confusion_matrix(y_train, yy_pred)\n",
    "#recall_train = recall_score(y_train,yy_pred,average='micro')\n",
    "#precision_train = precision_score(y_train,yy_pred,average='micro')\n",
    "#f1_score_train = f1_score(y_train,yy_pred,average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4320,    4,   16,   37],\n",
       "       [  11, 4145,   26,   22],\n",
       "       [  32,   19, 4335,   14],\n",
       "       [   8,   14,   19, 4519]], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9883321894303363 0.9911525585844094 0.9861237488626023 0.984102787456446\n"
     ]
    }
   ],
   "source": [
    "#recall train\n",
    "recall_R_train=cm_train[0][0]/(cm_train[0][0]+cm_train[1][0]+cm_train[2][0]+cm_train[3][0])\n",
    "recall_L_train=cm_train[1][1]/(cm_train[0][1]+cm_train[1][1]+cm_train[2][1]+cm_train[3][1])\n",
    "recall_F_train=cm_train[2][2]/(cm_train[0][2]+cm_train[1][2]+cm_train[2][2]+cm_train[3][2])\n",
    "recall_B_train=cm_train[3][3]/(cm_train[0][3]+cm_train[1][3]+cm_train[2][3]+cm_train[3][3])\n",
    "print(recall_R_train,recall_L_train,recall_F_train,recall_B_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9869773817683345 0.985965746907707 0.9852272727272727 0.9910087719298246\n"
     ]
    }
   ],
   "source": [
    "#precision train\n",
    "precision_R_train=cm_train[0][0]/(cm_train[0][0]+cm_train[0][1]+cm_train[0][2]+cm_train[0][3])\n",
    "precision_L_train=cm_train[1][1]/(cm_train[1][0]+cm_train[1][1]+cm_train[1][2]+cm_train[1][3])\n",
    "precision_F_train=cm_train[2][2]/(cm_train[2][0]+cm_train[2][1]+cm_train[2][2]+cm_train[2][3])\n",
    "precision_B_train=cm_train[3][3]/(cm_train[3][0]+cm_train[3][1]+cm_train[3][2]+cm_train[3][3])\n",
    "print(precision_R_train,precision_L_train,precision_F_train,precision_B_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9876543209876542 0.9885523491533509 0.985675306957708 0.9875437062937062\n"
     ]
    }
   ],
   "source": [
    "#f1score train\n",
    "f1score_R_train=2/((1/precision_R_train)+1/recall_R_train)\n",
    "f1score_L_train=2/((1/precision_L_train)+1/recall_L_train)\n",
    "f1score_F_train=2/((1/precision_F_train)+1/recall_F_train)\n",
    "f1score_B_train=2/((1/precision_B_train)+1/recall_B_train)\n",
    "print(f1score_R_train,f1score_L_train,f1score_F_train,f1score_B_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9873439370617411"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#accuary train\n",
    "accuracy_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAB1-2\n",
    "# LAB1-2\n",
    "# LAB1-2\n",
    "# LAB1-2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "# 刪除既有模型變數\n",
    "#del model \n",
    "\n",
    "# 載入模型\n",
    "model = load_model('classifier_lab1102.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#看模型內的資料(只是測試可跳過)\n",
    "# import numpy as np\n",
    "# from time import perf_counter\n",
    "# from keras.preprocessing import image\n",
    "# total_time1=0\n",
    "# s = time.perf_counter()\n",
    "# img = image.load_img(''), target_size = (img_width, img_height))\n",
    "# e= time.perf_counter()\n",
    "# print()\n",
    "\n",
    "# total_time=0\n",
    "# img_width, img_height = 32, 32\n",
    "# paths=['R_train','L_train','F_train','B_train']\n",
    "# for path in paths:\n",
    "#     files= os.listdir(path) #得到資料夾下的所有檔名稱\n",
    "#     A=[]\n",
    "#     for file in files: #遍歷資料夾\n",
    "#         start = time.perf_counter()\n",
    "#         img = image.load_img(path+'/'+file, target_size = (img_width, img_height))\n",
    "#         img = image.img_to_array(img)\n",
    "#         img = np.expand_dims(img, axis = 0)\n",
    "        \n",
    "#         pred=model.predict(img)[0]\n",
    "#         end = time.perf_counter()\n",
    "#         total=end-start\n",
    "#         total_time=total_time+total\n",
    "#         #print(pred)\n",
    "#         list1=pred.tolist()\n",
    "#         Ans=list1.index(max(list1))\n",
    "#         A.append(map_characters[Ans])\n",
    "#         #print(map_characters[Ans])\n",
    "#     print(\"cnt=\",len(A),\"\\ncnt_R=\",A.count('R_train'),\"\\ncnt_L=\",A.count('L_train'),\"\\ncnt_F=\",A.count('F_train'),\"\\ncnt_B=\",A.count('B_train'),\"\\ntotal_time=\",total_time,\"\\naverage_time=\",total_time/len(A))\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lab1-2/R/LINE_ALBUM_R_211103_0.jpg  spent:  0.01690839999992022\n",
      "Lab1-2/R/LINE_ALBUM_R_211103_0.jpg  =  Face_R \n",
      "\n",
      "Lab1-2/R/LINE_ALBUM_R_211103_9.jpg  spent:  0.007449600000200007\n",
      "Lab1-2/R/LINE_ALBUM_R_211103_9.jpg  =  Face_R \n",
      " \n",
      " \n",
      "\n",
      "cnt= 29 \n",
      "cnt_R= 15 \n",
      "cnt_L= 2 \n",
      "cnt_F= 8 \n",
      "cnt_B= 4 \n",
      "total_time= 0.5739438000009613 \n",
      "average_time= 0.019791165517274528\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#我們拍的資料\n",
    "import numpy as np\n",
    "from time import perf_counter\n",
    "from keras.preprocessing import image\n",
    "\n",
    "to1=0\n",
    "pathh = 'Lab1-2/R/LINE_ALBUM_R_211103_0.jpg'\n",
    "s = time.perf_counter()\n",
    "img = image.load_img(pathh, target_size = (32, 32))\n",
    "e= time.perf_counter()\n",
    "print(pathh,\" spent: \",e-s)\n",
    "print(pathh,\" = \",\"Face_R\",\"\\n\")\n",
    "\n",
    "\n",
    "pathh = 'Lab1-2/R/LINE_ALBUM_R_211103_9.jpg'\n",
    "s1 = time.perf_counter()\n",
    "img = image.load_img(pathh, target_size = (32, 32))\n",
    "e1= time.perf_counter()\n",
    "print(pathh,\" spent: \",e1-s1)\n",
    "print(pathh,\" = \",\"Face_R\",\"\\n\",\"\\n\",\"\\n\")\n",
    "\n",
    "total_time=0\n",
    "img_width, img_height = 32, 32\n",
    "paths=['Lab1-2/R/']\n",
    "for path in paths:\n",
    "    files= os.listdir(path) #得到資料夾下的所有檔名稱\n",
    "    A=[]\n",
    "    for file in files: #遍歷資料夾\n",
    "        start = time.perf_counter()\n",
    "        img = image.load_img(path+'/'+file, target_size = (img_width, img_height))\n",
    "        img = image.img_to_array(img)\n",
    "        img = np.expand_dims(img, axis = 0)\n",
    "\n",
    "        pred=model.predict(img)[0]\n",
    "        #print(pred)\n",
    "        end = time.perf_counter()\n",
    "        total=end-start\n",
    "        total_time=total_time+total\n",
    "        list1=pred.tolist()\n",
    "        Ans=list1.index(max(list1))\n",
    "        A.append(map_characters[Ans])\n",
    "        #print(map_characters[Ans])\n",
    "    print(\"cnt=\",len(A),\"\\ncnt_R=\",A.count('R_train'),\"\\ncnt_L=\",A.count('L_train'),\"\\ncnt_F=\",A.count('F_train'),\"\\ncnt_B=\",A.count('B_train'),\"\\ntotal_time=\",total_time,\"\\naverage_time=\",total_time/len(A))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lab1-2/L/LINE_ALBUM_L_211103_0.jpg  spent:  0.011539000000084343\n",
      "Lab1-2/L/LINE_ALBUM_L_211103_0.jpg  =  Face_L \n",
      "\n",
      "Lab1-2/L/LINE_ALBUM_L_211103_4_0.jpg  spent:  0.012322600000061357\n",
      "Lab1-2/L/LINE_ALBUM_L_211103_4_0.jpg  =  Face_L \n",
      " \n",
      " \n",
      "\n",
      "cnt= 32 \n",
      "cnt_R= 1 \n",
      "cnt_L= 22 \n",
      "cnt_F= 7 \n",
      "cnt_B= 2 \n",
      "total_time= 0.5545626999996784 \n",
      "average_time= 0.01733008437498995\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#我們拍的資料\n",
    "import numpy as np\n",
    "from time import perf_counter\n",
    "from keras.preprocessing import image\n",
    "\n",
    "to1=0\n",
    "pathh = 'Lab1-2/L/LINE_ALBUM_L_211103_0.jpg'\n",
    "s = time.perf_counter()\n",
    "img = image.load_img(pathh, target_size = (32, 32))\n",
    "e= time.perf_counter()\n",
    "print(pathh,\" spent: \",e-s)\n",
    "print(pathh,\" = \",\"Face_L\",\"\\n\")\n",
    "\n",
    "\n",
    "pathh = 'Lab1-2/L/LINE_ALBUM_L_211103_4_0.jpg'\n",
    "s1 = time.perf_counter()\n",
    "img = image.load_img(pathh, target_size = (32, 32))\n",
    "e1= time.perf_counter()\n",
    "print(pathh,\" spent: \",e1-s1)\n",
    "print(pathh,\" = \",\"Face_L\",\"\\n\",\"\\n\",\"\\n\")\n",
    "\n",
    "total_time=0\n",
    "img_width, img_height = 32, 32\n",
    "paths=['Lab1-2/L']\n",
    "for path in paths:\n",
    "    files= os.listdir(path) #得到資料夾下的所有檔名稱\n",
    "    A=[]\n",
    "    for file in files: #遍歷資料夾\n",
    "        start = time.perf_counter()\n",
    "        img = image.load_img(path+'/'+file, target_size = (img_width, img_height))\n",
    "        img = image.img_to_array(img)\n",
    "        img = np.expand_dims(img, axis = 0)\n",
    "\n",
    "        pred=model.predict(img)[0]\n",
    "        #print(pred)\n",
    "        end = time.perf_counter()\n",
    "        total=end-start\n",
    "        total_time=total_time+total\n",
    "        list1=pred.tolist()\n",
    "        Ans=list1.index(max(list1))\n",
    "        A.append(map_characters[Ans])\n",
    "        #print(map_characters[Ans])\n",
    "    print(\"cnt=\",len(A),\"\\ncnt_R=\",A.count('R_train'),\"\\ncnt_L=\",A.count('L_train'),\"\\ncnt_F=\",A.count('F_train'),\"\\ncnt_B=\",A.count('B_train'),\"\\ntotal_time=\",total_time,\"\\naverage_time=\",total_time/len(A))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lab1-2/F/LINE_ALBUM_F_211103_0.jpg  spent:  0.010295599999608385\n",
      "Lab1-2/F/LINE_ALBUM_F_211103_0.jpg  =  Face_F \n",
      "\n",
      "Lab1-2/F/LINE_ALBUM_F_211103_24.jpg  spent:  0.0071975999999267515\n",
      "Lab1-2/F/LINE_ALBUM_F_211103_24.jpg  =  Face_F \n",
      " \n",
      " \n",
      "\n",
      "cnt= 31 \n",
      "cnt_R= 2 \n",
      "cnt_L= 3 \n",
      "cnt_F= 26 \n",
      "cnt_B= 0 \n",
      "total_time= 0.4596196000006785 \n",
      "average_time= 0.014826438709699305\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#我們拍的資料\n",
    "import numpy as np\n",
    "from time import perf_counter\n",
    "from keras.preprocessing import image\n",
    "\n",
    "to1=0\n",
    "pathh = 'Lab1-2/F/LINE_ALBUM_F_211103_0.jpg'\n",
    "s = time.perf_counter()\n",
    "img = image.load_img(pathh, target_size = (32, 32))\n",
    "e= time.perf_counter()\n",
    "print(pathh,\" spent: \",e-s)\n",
    "print(pathh,\" = \",\"Face_F\",\"\\n\")\n",
    "\n",
    "\n",
    "pathh = 'Lab1-2/F/LINE_ALBUM_F_211103_24.jpg'\n",
    "s1 = time.perf_counter()\n",
    "img = image.load_img(pathh, target_size = (32, 32))\n",
    "e1= time.perf_counter()\n",
    "print(pathh,\" spent: \",e1-s1)\n",
    "print(pathh,\" = \",\"Face_F\",\"\\n\",\"\\n\",\"\\n\")\n",
    "\n",
    "total_time=0\n",
    "img_width, img_height = 32, 32\n",
    "paths=['Lab1-2/F']\n",
    "for path in paths:\n",
    "    files= os.listdir(path) #得到資料夾下的所有檔名稱\n",
    "    A=[]\n",
    "    for file in files: #遍歷資料夾\n",
    "        start = time.perf_counter()\n",
    "        img = image.load_img(path+'/'+file, target_size = (img_width, img_height))\n",
    "        img = image.img_to_array(img)\n",
    "        img = np.expand_dims(img, axis = 0)\n",
    "\n",
    "        pred=model.predict(img)[0]\n",
    "        #print(pred)\n",
    "        end = time.perf_counter()\n",
    "        total=end-start\n",
    "        total_time=total_time+total\n",
    "        list1=pred.tolist()\n",
    "        Ans=list1.index(max(list1))\n",
    "        A.append(map_characters[Ans])\n",
    "        #print(map_characters[Ans])\n",
    "    print(\"cnt=\",len(A),\"\\ncnt_R=\",A.count('R_train'),\"\\ncnt_L=\",A.count('L_train'),\"\\ncnt_F=\",A.count('F_train'),\"\\ncnt_B=\",A.count('B_train'),\"\\ntotal_time=\",total_time,\"\\naverage_time=\",total_time/len(A))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lab1-2/B/LINE_ALBUM_B_211103_0.jpg  spent:  0.035618300000351155\n",
      "Lab1-2/B/LINE_ALBUM_B_211103_0.jpg  =  Face_B \n",
      "\n",
      "Lab1-2/B/LINE_ALBUM_B_211103_30.jpg  spent:  0.012592399999903137\n",
      "Lab1-2/B/LINE_ALBUM_B_211103_30.jpg  =  Face_B \n",
      " \n",
      " \n",
      "\n",
      "cnt= 31 \n",
      "cnt_R= 5 \n",
      "cnt_L= 5 \n",
      "cnt_F= 7 \n",
      "cnt_B= 14 \n",
      "total_time= 0.8859118000009403 \n",
      "average_time= 0.02857780000003033\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#我們拍的資料\n",
    "import numpy as np\n",
    "from time import perf_counter\n",
    "from keras.preprocessing import image\n",
    "\n",
    "\n",
    "to1=0\n",
    "pathh = 'Lab1-2/B/LINE_ALBUM_B_211103_0.jpg'\n",
    "s = time.perf_counter()\n",
    "img = image.load_img(pathh, target_size = (32, 32))\n",
    "e= time.perf_counter()\n",
    "print(pathh,\" spent: \",e-s)\n",
    "print(pathh,\" = \",\"Face_B\",\"\\n\")\n",
    "\n",
    "\n",
    "pathh = 'Lab1-2/B/LINE_ALBUM_B_211103_30.jpg'\n",
    "s1 = time.perf_counter()\n",
    "img = image.load_img(pathh, target_size = (32, 32))\n",
    "e1= time.perf_counter()\n",
    "print(pathh,\" spent: \",e1-s1)\n",
    "print(pathh,\" = \",\"Face_B\",\"\\n\",\"\\n\",\"\\n\")\n",
    "\n",
    "total_time=0\n",
    "img_width, img_height = 32, 32\n",
    "paths=['Lab1-2/B']\n",
    "for path in paths:\n",
    "    files= os.listdir(path) #得到資料夾下的所有檔名稱\n",
    "    A=[]\n",
    "    for file in files: #遍歷資料夾\n",
    "        start = time.perf_counter()\n",
    "        img = image.load_img(path+'/'+file, target_size = (img_width, img_height))\n",
    "        img = image.img_to_array(img)\n",
    "        img = np.expand_dims(img, axis = 0)\n",
    "\n",
    "        pred=model.predict(img)[0]\n",
    "        #print(pred)\n",
    "        end = time.perf_counter()\n",
    "        total=end-start\n",
    "        total_time=total_time+total\n",
    "        list1=pred.tolist()\n",
    "        Ans=list1.index(max(list1))\n",
    "        A.append(map_characters[Ans])\n",
    "        #print(map_characters[Ans])\n",
    "    print(\"cnt=\",len(A),\"\\ncnt_R=\",A.count('R_train'),\"\\ncnt_L=\",A.count('L_train'),\"\\ncnt_F=\",A.count('F_train'),\"\\ncnt_B=\",A.count('B_train'),\"\\ntotal_time=\",total_time,\"\\naverage_time=\",total_time/len(A))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
